{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz5omuE9ypy8",
        "outputId": "6081852d-98c5-4414-ee35-6be142641e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: farama-notifications, Mako, gymnasium, colorlog, alembic, stable-baselines3, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 colorlog-6.8.2 farama-notifications-0.0.4 gymnasium-0.29.1 optuna-4.0.0 stable-baselines3-2.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna gymnasium stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_wkTJR2yIWa"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DeepSeaEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Deep Sea Exploration Environment\n",
        "\n",
        "    The agent moves through an NxN grid starting from the top left and trying to reach\n",
        "    the bottom right. Moving right yields higher rewards, while moving left incurs penalties.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, N=5):\n",
        "        super(DeepSeaEnv, self).__init__()\n",
        "        self.N = N\n",
        "\n",
        "        # Define action and observation spaces\n",
        "        self.action_space = spaces.Discrete(2)  # 0: Left, 1: Right\n",
        "        self.observation_space = spaces.Discrete(N * N)  # Flattened grid\n",
        "\n",
        "        # Reward constants\n",
        "        self.penalty = -0.01 / N\n",
        "        self.final_reward = 1.0\n",
        "\n",
        "        # Initialize state\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        self.state = (0, 0)  # Start at the top-left corner\n",
        "        self.steps_taken = 0\n",
        "        return self._get_obs(), {}  # Return flattened state and an empty info dict\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"Convert 2D coordinates to a single integer.\"\"\"\n",
        "        row, col = self.state\n",
        "        return row * self.N + col\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take a step in the environment.\n",
        "\n",
        "        Parameters:\n",
        "            action (int): 0 for left, 1 for right.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (obs, reward, terminated, truncated, info)\n",
        "        \"\"\"\n",
        "        row, col = self.state\n",
        "\n",
        "        # Determine new column based on action\n",
        "        if action == 1:  # Right\n",
        "            new_col = min(col + 1, self.N - 1)\n",
        "            reward = self.penalty  # Small negative reward per step\n",
        "        else:  # Left\n",
        "            new_col = max(col - 1, 0)\n",
        "            reward = 0  # No penalty for left\n",
        "\n",
        "        # Move down one row\n",
        "        new_row = row + 1\n",
        "\n",
        "        self.state = (new_row, new_col)\n",
        "        self.steps_taken += 1\n",
        "\n",
        "        # Check if we have reached the terminal state\n",
        "        terminated = self.steps_taken == self.N - 1\n",
        "        truncated = False\n",
        "\n",
        "        # If at the bottom-right corner and all actions were right\n",
        "        if terminated and new_col == self.N - 1:\n",
        "            reward += self.final_reward\n",
        "\n",
        "        return self._get_obs(), reward, terminated, truncated, {}\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the environment (not implemented for simplicity).\"\"\"\n",
        "        grid = np.zeros((self.N, self.N))\n",
        "        row, col = self.state\n",
        "        grid[row, col] = 1  # Mark the agent's position\n",
        "\n",
        "        print(grid)\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# Register the environment\n",
        "gym.register(\n",
        "    id='DeepSea-v0',\n",
        "    entry_point=DeepSeaEnv,\n",
        "    kwargs={'N': 7},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY0VlY8-0qk7"
      },
      "source": [
        "My Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZivq_3c0nwV",
        "outputId": "4ecd3067-22c4-4bf2-dc54-142312bb9882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "N=7\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def normalize_slices(tensor):\n",
        "    return tensor / tensor.sum(dim=-1, keepdim=True)\n",
        "\n",
        "def normalise_initial(counts):\n",
        "    return counts / counts.sum()\n",
        "\n",
        "def softmax_policy(policy_table):\n",
        "    return torch.nn.functional.softmax(policy_table, dim=-1)\n",
        "\n",
        "def state_to_index(state, env):\n",
        "    if isinstance(env.observation_space, gym.spaces.MultiDiscrete):\n",
        "        # Calculate the index for MultiDiscrete space\n",
        "        index = 0\n",
        "        for i, (s, n) in enumerate(zip(state, env.observation_space.nvec)):\n",
        "            index += s * np.prod(env.observation_space.nvec[i+1:])\n",
        "        return int(index)\n",
        "    elif isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        return state\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported observation space type\")\n",
        "\n",
        "def get_num_states(env):\n",
        "    if isinstance(env.observation_space, gym.spaces.MultiDiscrete):\n",
        "        return np.prod(env.observation_space.nvec)\n",
        "    elif isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        return env.observation_space.n\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported observation space type\")\n",
        "\n",
        "def sample_steps(env, policy, num_steps, max_steps_per_episode):\n",
        "    num_states = get_num_states(env)\n",
        "    num_actions = env.action_space.n\n",
        "    trajectories = []\n",
        "    initial_states = []\n",
        "    transition_counts = torch.ones((num_states, num_actions, num_states), dtype=torch.int32, device=device)\n",
        "    reward_total = torch.zeros((num_states, num_actions), device=device)\n",
        "    reward_count = torch.zeros((num_states, num_actions), device=device)\n",
        "    initial_state_count = torch.zeros(num_states, dtype=torch.int32, device=device)\n",
        "\n",
        "    steps_taken = 0\n",
        "    while steps_taken < num_steps:\n",
        "        state, _ = env.reset()\n",
        "        #state_idx = state_to_index(state, env)\n",
        "        state_idx = state\n",
        "        initial_state_count[state_idx] += 1\n",
        "        initial_states.append(state_idx)\n",
        "        trajectory = []\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            action = torch.multinomial(policy[state_idx].cpu(), 1).item()\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            #next_state_idx = state_to_index(next_state, env)\n",
        "            next_state_idx = next_state\n",
        "            trajectory.append((state_idx, action, reward, next_state_idx))\n",
        "\n",
        "            transition_counts[state_idx, action, next_state_idx] += 1\n",
        "            reward_total[state_idx, action] += reward\n",
        "            reward_count[state_idx, action] += 1\n",
        "\n",
        "            steps_taken += 1\n",
        "            if done or steps_taken >= num_steps:\n",
        "                break\n",
        "            state_idx = next_state_idx\n",
        "\n",
        "        trajectories.append(trajectory)\n",
        "\n",
        "    return transition_counts, reward_total, reward_count, initial_state_count, initial_states, trajectories, steps_taken\n",
        "\n",
        "def process_trajectories(trajectories):\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    next_states = []\n",
        "\n",
        "    for trajectory in trajectories:\n",
        "        for step in trajectory:\n",
        "            states.append(step[0])\n",
        "            actions.append(step[1])\n",
        "            rewards.append(step[2])\n",
        "            next_states.append(step[3])\n",
        "\n",
        "    return (torch.tensor(states, device=device),\n",
        "            torch.tensor(actions, device=device),\n",
        "            torch.tensor(rewards, dtype=torch.float32, device=device),\n",
        "            torch.tensor(next_states, device=device))\n",
        "\n",
        "def compute_J_counting(env, policy, v, R, P, gamma=0.99):\n",
        "    num_states = P.shape[0]\n",
        "    P_a = P.permute(1, 0, 2)\n",
        "    P_pi = torch.einsum('sa,ask->sk', policy, P_a)\n",
        "    R_pi = torch.einsum('sa,sa->s', policy, R)\n",
        "\n",
        "    J = v.unsqueeze(0) @ torch.linalg.solve(torch.eye(num_states, device=device) - gamma * P_pi, R_pi.unsqueeze(1))\n",
        "\n",
        "    return J\n",
        "\n",
        "def tabular_feature_map(total_states, total_actions, regularizer, policy, initial_states, current_states, current_actions, next_states, rewards, gamma):\n",
        "    sample_size = len(current_states)\n",
        "    latent_dim = total_states * total_actions\n",
        "    initial_state_sample_size = len(initial_states)\n",
        "\n",
        "    # Create X more efficiently\n",
        "    X = torch.zeros(sample_size, latent_dim, device=device)\n",
        "    indices = current_states * total_actions + current_actions\n",
        "    X.scatter_(1, indices.unsqueeze(1), 1)\n",
        "\n",
        "    Y = torch.zeros(sample_size, latent_dim, device=device)\n",
        "    next_state_indices = next_states[:, None] * total_actions + torch.arange(total_actions, device=device)\n",
        "    Y[torch.arange(sample_size, device=device)[:, None], next_state_indices] = policy[next_states]\n",
        "\n",
        "    W = torch.zeros(latent_dim, device=device)\n",
        "    initial_state_indices = torch.tensor(initial_states, device=device)[:, None] * total_actions + torch.arange(total_actions, device=device)\n",
        "    W.index_add_(0, initial_state_indices.flatten(), policy[torch.tensor(initial_states, device=device)].flatten())\n",
        "    W /= initial_state_sample_size\n",
        "\n",
        "    # Compute C_lambda, D, and E in one go\n",
        "    C_lambda = X.T @ X + regularizer * torch.eye(latent_dim, device=device)\n",
        "    D = X.T @ Y\n",
        "    E = X.T @ rewards.unsqueeze(1)\n",
        "\n",
        "    # Solve linear systems\n",
        "    A = torch.linalg.solve(C_lambda, E).T\n",
        "    M = torch.linalg.solve(C_lambda, D).T\n",
        "\n",
        "    # Compute J\n",
        "    J = A @ torch.linalg.solve(torch.eye(latent_dim, device=device) - gamma * M, W)\n",
        "\n",
        "    return J\n",
        "\n",
        "class VectorizedAccumulatedData:\n",
        "    def __init__(self, max_size=int(N*10000*0.15), device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.max_size = max_size\n",
        "        self.device = device\n",
        "        self.transition_counts = None\n",
        "        self.reward_total = None\n",
        "        self.reward_count = None\n",
        "        self.initial_state_count = None\n",
        "        self.initial_states = deque(maxlen=max_size)\n",
        "        self.states = deque(maxlen=max_size)\n",
        "        self.actions = deque(maxlen=max_size)\n",
        "        self.rewards = deque(maxlen=max_size)\n",
        "        self.next_states = deque(maxlen=max_size)\n",
        "        self.total_steps = 0\n",
        "\n",
        "    def update(self, transition_counts, reward_total, reward_count, initial_state_count, initial_states, trajectories, steps):\n",
        "        # Update counts and totals\n",
        "        if self.transition_counts is None:\n",
        "            self.transition_counts = transition_counts.to(self.device)\n",
        "            self.reward_total = reward_total.to(self.device)\n",
        "            self.reward_count = reward_count.to(self.device)\n",
        "            self.initial_state_count = initial_state_count.to(self.device)\n",
        "        else:\n",
        "            self.transition_counts += transition_counts.to(self.device)\n",
        "            self.reward_total += reward_total.to(self.device)\n",
        "            self.reward_count += reward_count.to(self.device)\n",
        "            self.initial_state_count += initial_state_count.to(self.device)\n",
        "\n",
        "        # Update initial states\n",
        "        self.initial_states.extend(initial_states)\n",
        "\n",
        "        # Vectorized update of trajectory data\n",
        "        states, actions, rewards, next_states = zip(*[step for traj in trajectories for step in traj])\n",
        "        self.states.extend(states)\n",
        "        self.actions.extend(actions)\n",
        "        self.rewards.extend(rewards)\n",
        "        self.next_states.extend(next_states)\n",
        "\n",
        "        self.total_steps += steps\n",
        "\n",
        "        # Trim data if necessary\n",
        "        if self.total_steps > self.max_size:\n",
        "            excess = self.total_steps - self.max_size\n",
        "            for _ in range(excess):\n",
        "                self.states.popleft()\n",
        "                self.actions.popleft()\n",
        "                self.rewards.popleft()\n",
        "                self.next_states.popleft()\n",
        "            self.total_steps = self.max_size\n",
        "\n",
        "    def get_data(self):\n",
        "        return (\n",
        "            self.transition_counts,\n",
        "            self.reward_total,\n",
        "            self.reward_count,\n",
        "            self.initial_state_count,\n",
        "            list(self.initial_states),\n",
        "            torch.tensor(list(self.states), device=self.device),\n",
        "            torch.tensor(list(self.actions), device=self.device),\n",
        "            torch.tensor(list(self.rewards), device=self.device),\n",
        "            torch.tensor(list(self.next_states), device=self.device)\n",
        "        )\n",
        "\n",
        "    def process_trajectories(self):\n",
        "        return (\n",
        "            torch.tensor(list(self.states), device=self.device),\n",
        "            torch.tensor(list(self.actions), device=self.device),\n",
        "            torch.tensor(list(self.rewards), device=self.device),\n",
        "            torch.tensor(list(self.next_states), device=self.device)\n",
        "        )\n",
        "\n",
        "\n",
        "class CustomAlgorithm:\n",
        "    def __init__(self, env, method='tabular', batch_size=200, epochs_per_batch=10, lr=0.01, max_accumulated_steps=10000):\n",
        "        self.env = env\n",
        "        self.method = method\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs_per_batch = epochs_per_batch\n",
        "        self.lr = lr\n",
        "        self.max_accumulated_steps = max_accumulated_steps\n",
        "\n",
        "        self.total_states = get_num_states(env)\n",
        "        self.total_actions = env.action_space.n\n",
        "        self.gamma = 0.99\n",
        "        self.regularizer = 0.01\n",
        "\n",
        "        self.theta = torch.nn.Parameter(torch.ones(self.total_states, self.total_actions, device=device) / self.total_actions)\n",
        "        self.optimizer = optim.Adam([self.theta], lr=self.lr)\n",
        "\n",
        "        self.accumulated_data = VectorizedAccumulatedData(max_size=self.max_accumulated_steps, device=device)\n",
        "\n",
        "    def learn(self, total_timesteps):\n",
        "        steps_taken = 0\n",
        "        while steps_taken < total_timesteps:\n",
        "            # Data collection phase\n",
        "            with torch.no_grad():\n",
        "                policy = torch.nn.functional.softmax(self.theta, dim=1)\n",
        "                new_data = sample_steps(self.env, policy, self.batch_size, max_steps_per_episode=200)\n",
        "            self.accumulated_data.update(*new_data)\n",
        "            steps_taken += new_data[-1]\n",
        "\n",
        "            # Get accumulated data\n",
        "            transition_counts, reward_total, reward_count, initial_state_count, initial_states, states, actions, rewards_sample, next_states = self.accumulated_data.get_data()\n",
        "\n",
        "            v = normalise_initial(initial_state_count.float())\n",
        "            R = torch.div(reward_total, reward_count.where(reward_count != 0, torch.tensor(1.0, device=device)))\n",
        "            P = normalize_slices(transition_counts.float())\n",
        "\n",
        "            # Policy optimization phase\n",
        "            for _ in range(self.epochs_per_batch):\n",
        "                self.optimizer.zero_grad()\n",
        "                policy = torch.nn.functional.softmax(self.theta, dim=1)\n",
        "\n",
        "                if self.method == 'counting':\n",
        "                    J = compute_J_counting(self.env, policy, v, R, P, self.gamma)\n",
        "                elif self.method == 'tabular':\n",
        "                    J = tabular_feature_map(self.total_states, self.total_actions, self.regularizer, policy,\n",
        "                                            initial_states, states, actions, next_states, rewards_sample, self.gamma)\n",
        "                else:\n",
        "                    raise ValueError(\"method must be either 'counting' or 'tabular'\")\n",
        "\n",
        "                loss = -J\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, observation, state=None, deterministic=False):\n",
        "        with torch.no_grad():\n",
        "            policy = torch.nn.functional.softmax(self.theta, dim=1)\n",
        "            if deterministic:\n",
        "                action = policy[observation].argmax().item()\n",
        "            else:\n",
        "                action = torch.multinomial(policy[observation], 1).item()\n",
        "        return action, state\n",
        "\n",
        "def custom_algorithm(env, method='tabular', **kwargs):\n",
        "    return CustomAlgorithm(env, method=method, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw0nnlhJ26ao"
      },
      "source": [
        "REINFORCE Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MpBQl2H25yq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "class REINFORCEWrapper:\n",
        "    def __init__(self, env, learning_rate=0.01, gamma=0.99):\n",
        "        self.env = env\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.n_actions = env.action_space.n\n",
        "        self.N = env.N  # Assuming the environment has an attribute N for grid size\n",
        "        self.n_states = self.N * self.N\n",
        "        self.theta = np.zeros((self.n_states, self.n_actions))  # Policy parameters\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
        "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state_idx = state\n",
        "        action_probs = self.softmax(self.theta[state_idx])\n",
        "        return np.random.choice(self.n_actions, p=action_probs)\n",
        "\n",
        "    def update_policy(self, episode):\n",
        "        G = 0\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            state_idx = state\n",
        "            G = self.gamma * G + reward\n",
        "\n",
        "            action_probs = self.softmax(self.theta[state_idx])\n",
        "            grad = np.zeros_like(self.theta[state_idx])\n",
        "            grad[action] = 1 - action_probs[action]\n",
        "            grad -= action_probs\n",
        "\n",
        "            self.theta[state_idx] += self.lr * G * grad\n",
        "\n",
        "    def learn(self, total_timesteps):\n",
        "        steps_taken = 0\n",
        "        while steps_taken < total_timesteps:\n",
        "            state, _ = self.env.reset()\n",
        "            episode = []\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _, _ = self.env.step(action)\n",
        "                episode.append((state, action, reward))\n",
        "                state = next_state\n",
        "                steps_taken += 1\n",
        "                if steps_taken >= total_timesteps:\n",
        "                    break\n",
        "            self.update_policy(episode)\n",
        "        return self\n",
        "\n",
        "    def predict(self, observation, state=None, deterministic=False):\n",
        "        if deterministic:\n",
        "            action = np.argmax(self.softmax(self.theta[observation]))\n",
        "        else:\n",
        "            action = self.choose_action(observation)\n",
        "        return action, state\n",
        "\n",
        "def reinforce_algorithm(env, learning_rate=0.01, gamma=0.99):\n",
        "    return REINFORCEWrapper(env, learning_rate, gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4rY_wjTy9ML",
        "outputId": "ef3366e4-4ad4-4658-d9ab-385b10afd4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0, custom_algorithm, 0.005642857142857103\n",
            "1250\n",
            "1250, custom_algorithm, 0.0955714285714285\n",
            "2500\n",
            "2500, custom_algorithm, 0.2649285714285702\n",
            "3750\n",
            "3750, custom_algorithm, 0.5931571428571462\n",
            "5000\n",
            "5000, custom_algorithm, 0.7125285714285724\n",
            "6250\n",
            "6250, custom_algorithm, 0.8819571428571394\n",
            "7500\n",
            "7500, custom_algorithm, 0.8719285714285681\n",
            "8750\n",
            "8750, custom_algorithm, 0.9117428571428519\n",
            "10000\n",
            "10000, custom_algorithm, 0.8520714285714257\n",
            "11250\n",
            "11250, custom_algorithm, 0.8918571428571387\n",
            "12500\n",
            "12500, custom_algorithm, 0.9515571428571367\n",
            "13750\n",
            "13750, custom_algorithm, 0.9615857142857083\n",
            "15000\n",
            "15000, custom_algorithm, 0.9616571428571372\n",
            "16250\n",
            "16250, custom_algorithm, 0.9416999999999948\n",
            "17500\n",
            "17500, custom_algorithm, 0.971542857142851\n",
            "18750\n",
            "18750, custom_algorithm, 0.9615571428571372\n",
            "20000\n",
            "20000, custom_algorithm, 0.9914285714285649\n",
            "21250\n",
            "21250, custom_algorithm, 0.9614999999999939\n",
            "22500\n",
            "22500, custom_algorithm, 0.9914285714285649\n",
            "23750\n",
            "23750, custom_algorithm, 0.9616428571428512\n",
            "25000\n",
            "25000, custom_algorithm, 0.9914285714285649\n",
            "26250\n",
            "26250, custom_algorithm, 0.9715142857142794\n",
            "27500\n",
            "27500, custom_algorithm, 0.9914285714285649\n",
            "28750\n",
            "28750, custom_algorithm, 0.9814571428571364\n",
            "30000\n",
            "30000, custom_algorithm, 0.9914285714285649\n",
            "31250\n",
            "31250, custom_algorithm, 0.9715285714285653\n",
            "32500\n",
            "32500, custom_algorithm, 0.9814999999999937\n",
            "33750\n",
            "33750, custom_algorithm, 0.9815142857142795\n",
            "35000\n",
            "35000, custom_algorithm, 0.9715428571428509\n",
            "36250\n",
            "36250, custom_algorithm, 0.9814857142857081\n",
            "37500\n",
            "37500, custom_algorithm, 0.9914285714285649\n",
            "38750\n",
            "38750, custom_algorithm, 0.9914285714285649\n",
            "40000\n",
            "40000, custom_algorithm, 0.9914285714285649\n",
            "41250\n",
            "41250, custom_algorithm, 0.9814428571428506\n",
            "42500\n",
            "42500, custom_algorithm, 0.9715142857142797\n",
            "43750\n",
            "43750, custom_algorithm, 0.9814714285714221\n",
            "45000\n",
            "45000, custom_algorithm, 0.9914285714285649\n",
            "46250\n",
            "46250, custom_algorithm, 0.9914285714285649\n",
            "47500\n",
            "47500, custom_algorithm, 0.9815142857142795\n",
            "48750\n",
            "48750, custom_algorithm, 0.9914285714285649\n",
            "50000\n",
            "50000, custom_algorithm, 0.9814857142857081\n",
            "51250\n",
            "51250, custom_algorithm, -1.4285714285714285e-05\n",
            "52500\n",
            "52500, custom_algorithm, 0.9914285714285649\n",
            "53750\n",
            "53750, custom_algorithm, 0.9914285714285649\n",
            "55000\n",
            "55000, custom_algorithm, 0.9914285714285649\n",
            "56250\n",
            "56250, custom_algorithm, 0.9914285714285649\n",
            "57500\n",
            "57500, custom_algorithm, 0.9914285714285649\n",
            "58750\n",
            "58750, custom_algorithm, 0.9914285714285649\n",
            "60000\n",
            "60000, custom_algorithm, 0.9914285714285649\n",
            "61250\n",
            "61250, custom_algorithm, 0.9914285714285649\n",
            "62500\n",
            "62500, custom_algorithm, 0.9914285714285649\n",
            "63750\n",
            "63750, custom_algorithm, 0.9714999999999937\n",
            "65000\n",
            "65000, custom_algorithm, 0.9914285714285649\n",
            "66250\n",
            "66250, custom_algorithm, 0.9914285714285649\n",
            "67500\n",
            "67500, custom_algorithm, 0.9815142857142795\n",
            "68750\n",
            "68750, custom_algorithm, 0.9814857142857081\n",
            "70000\n",
            "70000, custom_algorithm, 0.9914285714285649\n",
            "0\n",
            "0, custom_algorithm, 0.005599999999999969\n",
            "1250\n",
            "1250, custom_algorithm, 0.08584285714285675\n",
            "2500\n",
            "2500, custom_algorithm, 0.314699999999998\n",
            "3750\n",
            "3750, custom_algorithm, 0.5436571428571454\n",
            "5000\n",
            "5000, custom_algorithm, 0.7624428571428566\n",
            "6250\n",
            "6250, custom_algorithm, 0.8023714285714273\n",
            "7500\n",
            "7500, custom_algorithm, 0.8720571428571396\n",
            "8750\n",
            "8750, custom_algorithm, 0.8918428571428536\n",
            "10000\n",
            "10000, custom_algorithm, 0.9316857142857097\n",
            "11250\n",
            "11250, custom_algorithm, 0.9117571428571385\n",
            "12500\n",
            "12500, custom_algorithm, 0.9715714285714224\n",
            "13750\n",
            "13750, custom_algorithm, 0.9814571428571367\n",
            "15000\n",
            "15000, custom_algorithm, 0.9217857142857094\n",
            "16250\n",
            "16250, custom_algorithm, 0.9615142857142797\n",
            "17500\n",
            "17500, custom_algorithm, 0.9715857142857083\n",
            "18750\n",
            "18750, custom_algorithm, 0.9416142857142803\n",
            "20000\n",
            "20000, custom_algorithm, 0.9515714285714231\n",
            "21250\n",
            "21250, custom_algorithm, 0.9814428571428507\n",
            "22500\n",
            "22500, custom_algorithm, 0.971499999999994\n",
            "23750\n",
            "23750, custom_algorithm, 0.9914285714285649\n",
            "25000\n",
            "25000, custom_algorithm, 0.9416142857142802\n",
            "26250\n",
            "26250, custom_algorithm, 0.971542857142851\n",
            "27500\n",
            "27500, custom_algorithm, 0.9814999999999937\n",
            "28750\n",
            "28750, custom_algorithm, 0.9914285714285649\n",
            "30000\n",
            "30000, custom_algorithm, 0.9814714285714223\n",
            "31250\n",
            "31250, custom_algorithm, 0.9914285714285649\n",
            "32500\n",
            "32500, custom_algorithm, 0.9914285714285649\n",
            "33750\n",
            "33750, custom_algorithm, 0.9914285714285649\n",
            "35000\n",
            "35000, custom_algorithm, 0.9715857142857083\n",
            "36250\n",
            "36250, custom_algorithm, 0.9914285714285649\n",
            "37500\n",
            "37500, custom_algorithm, 0.9616285714285656\n",
            "38750\n",
            "38750, custom_algorithm, 0.9914285714285649\n",
            "40000\n",
            "40000, custom_algorithm, 0.9914285714285649\n",
            "41250\n",
            "41250, custom_algorithm, 0.971599999999994\n",
            "42500\n",
            "42500, custom_algorithm, 0.9914285714285649\n",
            "43750\n",
            "43750, custom_algorithm, 0.9914285714285649\n",
            "45000\n",
            "45000, custom_algorithm, 0.9914285714285649\n",
            "46250\n",
            "46250, custom_algorithm, 0.9914285714285649\n",
            "47500\n",
            "47500, custom_algorithm, 0.9814714285714223\n",
            "48750\n",
            "48750, custom_algorithm, 0.9814714285714221\n",
            "50000\n",
            "50000, custom_algorithm, 0.9814571428571364\n",
            "51250\n",
            "51250, custom_algorithm, 0.9814428571428508\n",
            "52500\n",
            "52500, custom_algorithm, 0.9914285714285649\n",
            "53750\n",
            "53750, custom_algorithm, 0.9814857142857079\n",
            "55000\n",
            "55000, custom_algorithm, 0.9715285714285653\n",
            "56250\n",
            "56250, custom_algorithm, 0.9814714285714221\n",
            "57500\n",
            "57500, custom_algorithm, 0.9914285714285649\n",
            "58750\n",
            "58750, custom_algorithm, 0.9914285714285649\n",
            "60000\n",
            "60000, custom_algorithm, 0.9914285714285649\n",
            "61250\n",
            "61250, custom_algorithm, 0.9714999999999937\n",
            "62500\n",
            "62500, custom_algorithm, 0.9914285714285649\n",
            "63750\n",
            "63750, custom_algorithm, 0.9814999999999937\n",
            "65000\n",
            "65000, custom_algorithm, 0.9814428571428507\n",
            "66250\n",
            "66250, custom_algorithm, 0.9914285714285649\n",
            "67500\n",
            "67500, custom_algorithm, 0.9914285714285649\n",
            "68750\n",
            "68750, custom_algorithm, 0.9914285714285649\n",
            "70000\n",
            "70000, custom_algorithm, 0.9914285714285649\n",
            "0\n",
            "0, custom_algorithm, 0.015785714285714486\n",
            "1250\n",
            "1250, custom_algorithm, 0.06595714285714256\n",
            "2500\n",
            "2500, custom_algorithm, 0.2549571428571418\n",
            "3750\n",
            "3750, custom_algorithm, 0.5732000000000033\n",
            "5000\n",
            "5000, custom_algorithm, 0.72257142857143\n",
            "6250\n",
            "6250, custom_algorithm, 0.7825571428571425\n",
            "7500\n",
            "7500, custom_algorithm, 0.8720571428571395\n",
            "8750\n",
            "8750, custom_algorithm, -0.00044285714285714317\n",
            "10000\n",
            "10000, custom_algorithm, 0.9018857142857104\n",
            "11250\n",
            "11250, custom_algorithm, 0.9316571428571381\n",
            "12500\n",
            "12500, custom_algorithm, 0.9516285714285658\n",
            "13750\n",
            "13750, custom_algorithm, 0.9715142857142797\n",
            "15000\n",
            "15000, custom_algorithm, 0.971499999999994\n",
            "16250\n",
            "16250, custom_algorithm, 0.9715285714285655\n",
            "17500\n",
            "17500, custom_algorithm, 0.9714857142857083\n",
            "18750\n",
            "18750, custom_algorithm, 0.9715285714285655\n",
            "20000\n",
            "20000, custom_algorithm, 0.9417142857142803\n",
            "21250\n",
            "21250, custom_algorithm, 0.9715285714285653\n",
            "22500\n",
            "22500, custom_algorithm, 0.9715571428571369\n",
            "23750\n",
            "23750, custom_algorithm, 0.9814714285714221\n",
            "25000\n",
            "25000, custom_algorithm, 0.9715428571428509\n",
            "26250\n",
            "26250, custom_algorithm, 0.9714857142857081\n",
            "27500\n",
            "27500, custom_algorithm, 0.9714714285714227\n",
            "28750\n",
            "28750, custom_algorithm, 0.9715142857142796\n",
            "30000\n",
            "30000, custom_algorithm, 0.9914285714285649\n",
            "31250\n",
            "31250, custom_algorithm, 0.9914285714285649\n",
            "32500\n",
            "32500, custom_algorithm, 0.9814857142857079\n",
            "33750\n",
            "33750, custom_algorithm, 0.9914285714285649\n",
            "35000\n",
            "35000, custom_algorithm, 0.9715142857142794\n",
            "36250\n",
            "36250, custom_algorithm, 0.9914285714285649\n",
            "37500\n",
            "37500, custom_algorithm, 0.9815142857142795\n",
            "38750\n",
            "38750, custom_algorithm, 0.9814999999999937\n",
            "40000\n",
            "40000, custom_algorithm, 0.961657142857137\n",
            "41250\n",
            "41250, custom_algorithm, 0.9914285714285649\n",
            "42500\n",
            "42500, custom_algorithm, 0.9914285714285649\n",
            "43750\n",
            "43750, custom_algorithm, 0.9714857142857085\n",
            "45000\n",
            "45000, custom_algorithm, 0.9914285714285649\n",
            "46250\n",
            "46250, custom_algorithm, 0.9914285714285649\n",
            "47500\n",
            "47500, custom_algorithm, 0.9914285714285649\n",
            "48750\n",
            "48750, custom_algorithm, 0.9914285714285649\n",
            "50000\n",
            "50000, custom_algorithm, 0.9814714285714221\n",
            "51250\n",
            "51250, custom_algorithm, 0.9814714285714223\n",
            "52500\n",
            "52500, custom_algorithm, 0.9914285714285649\n",
            "53750\n",
            "53750, custom_algorithm, 0.9914285714285649\n",
            "55000\n",
            "55000, custom_algorithm, 0.9814857142857079\n",
            "56250\n",
            "56250, custom_algorithm, 0.9914285714285649\n",
            "57500\n",
            "57500, custom_algorithm, 0.9914285714285649\n",
            "58750\n",
            "58750, custom_algorithm, 0.9814571428571364\n",
            "60000\n",
            "60000, custom_algorithm, 0.9914285714285649\n",
            "61250\n",
            "61250, custom_algorithm, 0.9914285714285649\n",
            "62500\n",
            "62500, custom_algorithm, 0.9814999999999937\n",
            "63750\n",
            "63750, custom_algorithm, 0.9814857142857079\n",
            "65000\n",
            "65000, custom_algorithm, 0.9914285714285649\n",
            "66250\n",
            "66250, custom_algorithm, 0.9914285714285649\n",
            "67500\n",
            "67500, custom_algorithm, 0.9914285714285649\n",
            "68750\n",
            "68750, custom_algorithm, 0.9914285714285649\n",
            "70000\n",
            "70000, custom_algorithm, 0.9914285714285649\n",
            "0\n",
            "0, custom_algorithm, 0.035757142857143\n",
            "1250\n",
            "1250, custom_algorithm, 0.006757142857142819\n",
            "2500\n",
            "2500, custom_algorithm, 0.1054571428571428\n",
            "3750\n",
            "3750, custom_algorithm, 0.4736714285714306\n",
            "5000\n",
            "5000, custom_algorithm, 0.6225857142857188\n",
            "6250\n",
            "6250, custom_algorithm, 0.8320571428571408\n",
            "7500\n",
            "7500, custom_algorithm, 0.861714285714282\n",
            "8750\n",
            "8750, custom_algorithm, 0.9215857142857098\n",
            "10000\n",
            "10000, custom_algorithm, 0.9614714285714225\n",
            "11250\n",
            "11250, custom_algorithm, 0.9614999999999942\n",
            "12500\n",
            "12500, custom_algorithm, 0.961485714285708\n",
            "13750\n",
            "13750, custom_algorithm, 0.9316142857142811\n",
            "15000\n",
            "15000, custom_algorithm, 0.9714857142857078\n",
            "16250\n",
            "16250, custom_algorithm, 0.9814428571428508\n",
            "17500\n",
            "17500, custom_algorithm, 0.9614714285714223\n",
            "18750\n",
            "18750, custom_algorithm, 0.9814428571428506\n",
            "20000\n",
            "20000, custom_algorithm, 0.9814428571428506\n",
            "21250\n",
            "21250, custom_algorithm, 0.971499999999994\n",
            "22500\n",
            "22500, custom_algorithm, 0.9814428571428508\n",
            "23750\n",
            "23750, custom_algorithm, 0.9914285714285649\n",
            "25000\n",
            "25000, custom_algorithm, 0.9814428571428507\n",
            "26250\n",
            "26250, custom_algorithm, 0.9914285714285649\n",
            "27500\n",
            "27500, custom_algorithm, 0.9814428571428506\n",
            "28750\n",
            "28750, custom_algorithm, 0.9914285714285649\n",
            "30000\n",
            "30000, custom_algorithm, 0.9814428571428508\n",
            "31250\n",
            "31250, custom_algorithm, 0.9814428571428506\n",
            "32500\n",
            "32500, custom_algorithm, 0.9914285714285649\n",
            "33750\n",
            "33750, custom_algorithm, 0.9914285714285649\n",
            "35000\n",
            "35000, custom_algorithm, -2.857142857142857e-05\n",
            "36250\n",
            "36250, custom_algorithm, 0.9814428571428507\n",
            "37500\n",
            "37500, custom_algorithm, 0.9914285714285649\n",
            "38750\n",
            "38750, custom_algorithm, 0.9714714285714223\n",
            "40000\n",
            "40000, custom_algorithm, 0.9814428571428506\n",
            "41250\n",
            "41250, custom_algorithm, 0.9914285714285649\n",
            "42500\n",
            "42500, custom_algorithm, 0.9914285714285649\n",
            "43750\n",
            "43750, custom_algorithm, 0.9914285714285649\n",
            "45000\n",
            "45000, custom_algorithm, 0.9914285714285649\n",
            "46250\n",
            "46250, custom_algorithm, 0.9914285714285649\n",
            "47500\n",
            "47500, custom_algorithm, 0.9914285714285649\n",
            "48750\n",
            "48750, custom_algorithm, 0.9914285714285649\n",
            "50000\n",
            "50000, custom_algorithm, 0.9914285714285649\n",
            "51250\n",
            "51250, custom_algorithm, 0.9914285714285649\n",
            "52500\n",
            "52500, custom_algorithm, 0.9914285714285649\n",
            "53750\n",
            "53750, custom_algorithm, 0.9914285714285649\n",
            "55000\n",
            "55000, custom_algorithm, 0.9814999999999937\n",
            "56250\n",
            "56250, custom_algorithm, 0.9914285714285649\n",
            "57500\n",
            "57500, custom_algorithm, 0.0\n",
            "58750\n",
            "58750, custom_algorithm, 0.9914285714285649\n",
            "60000\n",
            "60000, custom_algorithm, 0.9814714285714223\n",
            "61250\n",
            "61250, custom_algorithm, 0.9914285714285649\n",
            "62500\n",
            "62500, custom_algorithm, 0.9814857142857079\n",
            "63750\n",
            "63750, custom_algorithm, 0.9914285714285649\n",
            "65000\n",
            "65000, custom_algorithm, 0.9914285714285649\n",
            "66250\n",
            "66250, custom_algorithm, 0.9914285714285649\n",
            "67500\n",
            "67500, custom_algorithm, 0.9814571428571365\n",
            "68750\n",
            "68750, custom_algorithm, 0.9914285714285649\n",
            "70000\n",
            "70000, custom_algorithm, 0.9914285714285649\n",
            "0\n",
            "0, custom_algorithm, 0.03565714285714302\n",
            "1250\n",
            "1250, custom_algorithm, 0.016514285714285742\n",
            "2500\n",
            "2500, custom_algorithm, 0.13560000000000083\n",
            "3750\n",
            "3750, custom_algorithm, 0.3541571428571423\n",
            "5000\n",
            "5000, custom_algorithm, 0.5928428571428607\n",
            "6250\n",
            "6250, custom_algorithm, 0.7720571428571418\n",
            "7500\n",
            "7500, custom_algorithm, 0.8816999999999957\n",
            "8750\n",
            "8750, custom_algorithm, 0.8916857142857094\n",
            "10000\n",
            "10000, custom_algorithm, 0.961485714285709\n",
            "11250\n",
            "11250, custom_algorithm, 0.9315571428571379\n",
            "12500\n",
            "12500, custom_algorithm, 0.9614714285714225\n",
            "13750\n",
            "13750, custom_algorithm, 0.9514857142857086\n",
            "15000\n",
            "15000, custom_algorithm, 0.9714571428571368\n",
            "16250\n",
            "16250, custom_algorithm, 0.9914285714285649\n",
            "17500\n",
            "17500, custom_algorithm, 0.9714571428571365\n",
            "18750\n",
            "18750, custom_algorithm, 0.9914285714285649\n",
            "20000\n",
            "20000, custom_algorithm, 0.9914285714285649\n",
            "21250\n",
            "21250, custom_algorithm, 0.9714999999999937\n",
            "22500\n",
            "22500, custom_algorithm, 0.9914285714285649\n",
            "23750\n",
            "23750, custom_algorithm, 0.9714571428571367\n",
            "25000\n",
            "25000, custom_algorithm, 0.9714857142857082\n",
            "26250\n",
            "26250, custom_algorithm, 0.9914285714285649\n",
            "27500\n",
            "27500, custom_algorithm, 0.9814857142857081\n",
            "28750\n",
            "28750, custom_algorithm, 0.9814428571428508\n",
            "30000\n",
            "30000, custom_algorithm, 0.9814714285714223\n",
            "31250\n",
            "31250, custom_algorithm, 0.9715142857142796\n",
            "32500\n",
            "32500, custom_algorithm, 0.9914285714285649\n",
            "33750\n",
            "33750, custom_algorithm, 0.9914285714285649\n",
            "35000\n",
            "35000, custom_algorithm, 0.9914285714285649\n",
            "36250\n",
            "36250, custom_algorithm, -2.857142857142857e-05\n",
            "37500\n",
            "37500, custom_algorithm, 0.9914285714285649\n",
            "38750\n",
            "38750, custom_algorithm, 0.9914285714285649\n",
            "40000\n",
            "40000, custom_algorithm, 0.9914285714285649\n",
            "41250\n",
            "41250, custom_algorithm, 0.9914285714285649\n",
            "42500\n",
            "42500, custom_algorithm, 0.9914285714285649\n",
            "43750\n",
            "43750, custom_algorithm, 0.9914285714285649\n",
            "45000\n",
            "45000, custom_algorithm, 0.9615428571428513\n",
            "46250\n",
            "46250, custom_algorithm, 0.9914285714285649\n",
            "47500\n",
            "47500, custom_algorithm, 0.9814428571428506\n",
            "48750\n",
            "48750, custom_algorithm, 0.9914285714285649\n",
            "50000\n",
            "50000, custom_algorithm, 0.9914285714285649\n",
            "51250\n",
            "51250, custom_algorithm, 0.9914285714285649\n",
            "52500\n",
            "52500, custom_algorithm, 0.9914285714285649\n",
            "53750\n",
            "53750, custom_algorithm, 0.9814857142857081\n",
            "55000\n",
            "55000, custom_algorithm, 0.9914285714285649\n",
            "56250\n",
            "56250, custom_algorithm, 0.9914285714285649\n",
            "57500\n",
            "57500, custom_algorithm, 0.9914285714285649\n",
            "58750\n",
            "58750, custom_algorithm, 0.9914285714285649\n",
            "60000\n",
            "60000, custom_algorithm, 0.9914285714285649\n",
            "61250\n",
            "61250, custom_algorithm, 0.9914285714285649\n",
            "62500\n",
            "62500, custom_algorithm, 0.9914285714285649\n",
            "63750\n",
            "63750, custom_algorithm, 0.9914285714285649\n",
            "65000\n",
            "65000, custom_algorithm, 0.9914285714285649\n",
            "66250\n",
            "66250, custom_algorithm, 0.9914285714285649\n",
            "67500\n",
            "67500, custom_algorithm, 0.9814999999999937\n",
            "68750\n",
            "68750, custom_algorithm, 0.9914285714285649\n",
            "70000\n",
            "70000, custom_algorithm, 0.9914285714285649\n",
            "0\n",
            "0, custom_algorithm, 0.02578571428571441\n",
            "1250\n",
            "1250, custom_algorithm, 0.016757142857142915\n",
            "2500\n",
            "2500, custom_algorithm, 0.1256571428571432\n",
            "3750\n",
            "3750, custom_algorithm, 0.30437142857142685\n",
            "5000\n",
            "5000, custom_algorithm, 0.6524142857142903\n",
            "6250\n",
            "6250, custom_algorithm, 0.84175714285714\n",
            "7500\n",
            "7500, custom_algorithm, 0.9215999999999954\n",
            "8750\n",
            "8750, custom_algorithm, 0.8816571428571396\n",
            "10000\n",
            "10000, custom_algorithm, 0.9415428571428518\n",
            "11250\n",
            "11250, custom_algorithm, 0.9614999999999941\n",
            "12500\n",
            "12500, custom_algorithm, 0.9614999999999941\n",
            "13750\n",
            "13750, custom_algorithm, 0.9714571428571365\n",
            "15000\n",
            "15000, custom_algorithm, 0.9714571428571365\n",
            "16250\n",
            "16250, custom_algorithm, 0.96151428571428\n",
            "17500\n",
            "17500, custom_algorithm, 0.9914285714285649\n",
            "18750\n",
            "18750, custom_algorithm, 0.9714999999999939\n",
            "20000\n",
            "20000, custom_algorithm, 0.9814428571428507\n",
            "21250\n",
            "21250, custom_algorithm, 0.9714571428571368\n",
            "22500\n",
            "22500, custom_algorithm, 0.9814571428571364\n",
            "23750\n",
            "23750, custom_algorithm, 0.9714857142857083\n",
            "25000\n",
            "25000, custom_algorithm, 0.9914285714285649\n",
            "26250\n",
            "26250, custom_algorithm, 0.9714571428571364\n",
            "27500\n",
            "27500, custom_algorithm, 0.9814428571428506\n",
            "28750\n",
            "28750, custom_algorithm, 0.9814428571428506\n",
            "30000\n",
            "30000, custom_algorithm, 0.9914285714285649\n",
            "31250\n",
            "31250, custom_algorithm, 0.9914285714285649\n",
            "32500\n",
            "32500, custom_algorithm, 0.9914285714285649\n",
            "33750\n",
            "33750, custom_algorithm, 0.9814857142857079\n",
            "35000\n",
            "35000, custom_algorithm, 0.9914285714285649\n",
            "36250\n",
            "36250, custom_algorithm, 0.9914285714285649\n",
            "37500\n",
            "37500, custom_algorithm, 0.9814428571428508\n",
            "38750\n",
            "38750, custom_algorithm, 0.9914285714285649\n",
            "40000\n",
            "40000, custom_algorithm, 0.9914285714285649\n",
            "41250\n",
            "41250, custom_algorithm, -1.4285714285714285e-05\n",
            "42500\n",
            "42500, custom_algorithm, 0.9714571428571371\n",
            "43750\n",
            "43750, custom_algorithm, 0.9914285714285649\n",
            "45000\n",
            "45000, custom_algorithm, 0.9914285714285649\n",
            "46250\n",
            "46250, custom_algorithm, 0.9914285714285649\n",
            "47500\n",
            "47500, custom_algorithm, 0.9814428571428506\n",
            "48750\n",
            "48750, custom_algorithm, 0.9914285714285649\n",
            "50000\n",
            "50000, custom_algorithm, 0.9914285714285649\n",
            "51250\n",
            "51250, custom_algorithm, 0.9914285714285649\n",
            "52500\n",
            "52500, custom_algorithm, 0.9814428571428508\n",
            "53750\n",
            "53750, custom_algorithm, 0.9914285714285649\n",
            "55000\n",
            "55000, custom_algorithm, 0.9914285714285649\n",
            "56250\n",
            "56250, custom_algorithm, 0.9914285714285649\n",
            "57500\n",
            "57500, custom_algorithm, 0.9914285714285649\n",
            "58750\n",
            "58750, custom_algorithm, 0.9814857142857079\n",
            "60000\n",
            "60000, custom_algorithm, 0.9914285714285649\n",
            "61250\n",
            "61250, custom_algorithm, 0.9914285714285649\n",
            "62500\n",
            "62500, custom_algorithm, 0.9914285714285649\n",
            "63750\n",
            "63750, custom_algorithm, 0.9914285714285649\n",
            "65000\n",
            "65000, custom_algorithm, 0.9914285714285649\n",
            "66250\n",
            "66250, custom_algorithm, 0.9914285714285649\n",
            "67500\n",
            "67500, custom_algorithm, 0.9914285714285649\n",
            "68750\n",
            "68750, custom_algorithm, 0.9914285714285649\n",
            "70000\n",
            "70000, custom_algorithm, 0.9914285714285649\n",
            "0\n",
            "0, reinforce_algorithm, 0.0056285714285714125\n",
            "1250\n",
            "1250, reinforce_algorithm, -0.0042857142857143\n",
            "2500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.N to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.N` for environment variables or `env.get_wrapper_attr('N')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2500, reinforce_algorithm, 0.015471428571428723\n",
            "3750\n",
            "3750, reinforce_algorithm, 0.005542857142857091\n",
            "5000\n",
            "5000, reinforce_algorithm, 0.005514285714285679\n",
            "6250\n",
            "6250, reinforce_algorithm, -0.004142857142857156\n",
            "7500\n",
            "7500, reinforce_algorithm, 0.035614285714285925\n",
            "8750\n",
            "8750, reinforce_algorithm, 0.015742857142857284\n",
            "10000\n",
            "10000, reinforce_algorithm, 0.015771428571428672\n",
            "11250\n",
            "11250, reinforce_algorithm, 0.045542857142857066\n",
            "12500\n",
            "12500, reinforce_algorithm, -0.004514285714285731\n",
            "13750\n",
            "13750, reinforce_algorithm, 0.005185714285714265\n",
            "15000\n",
            "15000, reinforce_algorithm, 0.035257142857142994\n",
            "16250\n",
            "16250, reinforce_algorithm, 0.005657142857142812\n",
            "17500\n",
            "17500, reinforce_algorithm, -0.0040428571428571545\n",
            "18750\n",
            "18750, reinforce_algorithm, 0.025414285714285875\n",
            "20000\n",
            "20000, reinforce_algorithm, 0.05557142857142849\n",
            "21250\n",
            "21250, reinforce_algorithm, 0.05511428571428561\n",
            "22500\n",
            "22500, reinforce_algorithm, 0.045414285714285796\n",
            "23750\n",
            "23750, reinforce_algorithm, 0.03537142857142868\n",
            "25000\n",
            "25000, reinforce_algorithm, 0.04542857142857133\n",
            "26250\n",
            "26250, reinforce_algorithm, 0.055257142857142456\n",
            "27500\n",
            "27500, reinforce_algorithm, 0.025600000000000046\n",
            "28750\n",
            "28750, reinforce_algorithm, 0.045200000000000004\n",
            "30000\n",
            "30000, reinforce_algorithm, 0.07524285714285632\n",
            "31250\n",
            "31250, reinforce_algorithm, 0.06541428571428531\n",
            "32500\n",
            "32500, reinforce_algorithm, 0.06537142857142826\n",
            "33750\n",
            "33750, reinforce_algorithm, 0.06544285714285702\n",
            "35000\n",
            "35000, reinforce_algorithm, 0.11498571428571414\n",
            "36250\n",
            "36250, reinforce_algorithm, 0.07538571428571415\n",
            "37500\n",
            "37500, reinforce_algorithm, 0.035300000000000234\n",
            "38750\n",
            "38750, reinforce_algorithm, 0.09507142857142897\n",
            "40000\n",
            "40000, reinforce_algorithm, 0.055228571428571564\n",
            "41250\n",
            "41250, reinforce_algorithm, 0.06505714285714272\n",
            "42500\n",
            "42500, reinforce_algorithm, 0.06504285714285697\n",
            "43750\n",
            "43750, reinforce_algorithm, 0.06509999999999957\n",
            "45000\n",
            "45000, reinforce_algorithm, 0.07507142857142825\n",
            "46250\n",
            "46250, reinforce_algorithm, 0.06505714285714238\n",
            "47500\n",
            "47500, reinforce_algorithm, 0.06554285714285646\n",
            "48750\n",
            "48750, reinforce_algorithm, 0.09488571428571431\n",
            "50000\n",
            "50000, reinforce_algorithm, 0.04517142857142854\n",
            "51250\n",
            "51250, reinforce_algorithm, 0.09527142857142862\n",
            "52500\n",
            "52500, reinforce_algorithm, 0.08522857142857147\n",
            "53750\n",
            "53750, reinforce_algorithm, 0.0750142857142851\n",
            "55000\n",
            "55000, reinforce_algorithm, 0.08505714285714287\n",
            "56250\n",
            "56250, reinforce_algorithm, 0.08517142857142833\n",
            "57500\n",
            "57500, reinforce_algorithm, 0.05512857142857139\n",
            "58750\n",
            "58750, reinforce_algorithm, 0.035242857142857256\n",
            "60000\n",
            "60000, reinforce_algorithm, 0.045485714285714234\n",
            "61250\n",
            "61250, reinforce_algorithm, 0.10495714285714261\n",
            "62500\n",
            "62500, reinforce_algorithm, 0.07488571428571345\n",
            "63750\n",
            "63750, reinforce_algorithm, 0.044942857142856736\n",
            "65000\n",
            "65000, reinforce_algorithm, 0.12485714285714287\n",
            "66250\n",
            "66250, reinforce_algorithm, 0.0850999999999998\n",
            "67500\n",
            "67500, reinforce_algorithm, 0.06514285714285696\n",
            "68750\n",
            "68750, reinforce_algorithm, 0.07515714285714241\n",
            "70000\n",
            "70000, reinforce_algorithm, 0.10482857142857156\n",
            "0\n",
            "0, reinforce_algorithm, 0.01577142857142859\n",
            "1250\n",
            "1250, reinforce_algorithm, 0.01591428571428579\n",
            "2500\n",
            "2500, reinforce_algorithm, 0.03564285714285724\n",
            "3750\n",
            "3750, reinforce_algorithm, 0.01551428571428575\n",
            "5000\n",
            "5000, reinforce_algorithm, -0.004400000000000016\n",
            "6250\n",
            "6250, reinforce_algorithm, 0.015628571428571478\n",
            "7500\n",
            "7500, reinforce_algorithm, 0.015871428571428713\n",
            "8750\n",
            "8750, reinforce_algorithm, 0.025571428571428738\n",
            "10000\n",
            "10000, reinforce_algorithm, -0.004142857142857156\n",
            "11250\n",
            "11250, reinforce_algorithm, 0.005842857142857115\n",
            "12500\n",
            "12500, reinforce_algorithm, 0.04545714285714292\n",
            "13750\n",
            "13750, reinforce_algorithm, 0.025585714285714437\n",
            "15000\n",
            "15000, reinforce_algorithm, -0.004214285714285728\n",
            "16250\n",
            "16250, reinforce_algorithm, 0.03547142857142874\n",
            "17500\n",
            "17500, reinforce_algorithm, 0.04527142857142859\n",
            "18750\n",
            "18750, reinforce_algorithm, 0.015371428571428694\n",
            "20000\n",
            "20000, reinforce_algorithm, 0.03525714285714296\n",
            "21250\n",
            "21250, reinforce_algorithm, 0.025685714285714426\n",
            "22500\n",
            "22500, reinforce_algorithm, 0.025271428571428806\n",
            "23750\n",
            "23750, reinforce_algorithm, 0.055485714285713916\n",
            "25000\n",
            "25000, reinforce_algorithm, 0.04525714285714285\n",
            "26250\n",
            "26250, reinforce_algorithm, 0.0058857142857142745\n",
            "27500\n",
            "27500, reinforce_algorithm, 0.025414285714285893\n",
            "28750\n",
            "28750, reinforce_algorithm, 0.035442857142857415\n",
            "30000\n",
            "30000, reinforce_algorithm, 0.045285714285714256\n",
            "31250\n",
            "31250, reinforce_algorithm, 0.0753285714285711\n",
            "32500\n",
            "32500, reinforce_algorithm, 0.055528571428570934\n",
            "33750\n",
            "33750, reinforce_algorithm, 0.03551428571428589\n",
            "35000\n",
            "35000, reinforce_algorithm, 0.05521428571428505\n",
            "36250\n",
            "36250, reinforce_algorithm, 0.025242857142857355\n",
            "37500\n",
            "37500, reinforce_algorithm, 0.08475714285714316\n",
            "38750\n",
            "38750, reinforce_algorithm, 0.09507142857142849\n",
            "40000\n",
            "40000, reinforce_algorithm, 0.035185714285714556\n",
            "41250\n",
            "41250, reinforce_algorithm, 0.045257142857143016\n",
            "42500\n",
            "42500, reinforce_algorithm, 0.0454285714285716\n",
            "43750\n",
            "43750, reinforce_algorithm, 0.05555714285714232\n",
            "45000\n",
            "45000, reinforce_algorithm, 0.06489999999999976\n",
            "46250\n",
            "46250, reinforce_algorithm, 0.03547142857142875\n",
            "47500\n",
            "47500, reinforce_algorithm, 0.045671428571428384\n",
            "48750\n",
            "48750, reinforce_algorithm, 0.07524285714285689\n",
            "50000\n",
            "50000, reinforce_algorithm, 0.09502857142857143\n",
            "51250\n",
            "51250, reinforce_algorithm, 0.09478571428571433\n",
            "52500\n",
            "52500, reinforce_algorithm, 0.09481428571428566\n",
            "53750\n",
            "53750, reinforce_algorithm, 0.06541428571428555\n",
            "55000\n",
            "55000, reinforce_algorithm, 0.04512857142857171\n",
            "56250\n",
            "56250, reinforce_algorithm, 0.10514285714285718\n",
            "57500\n",
            "57500, reinforce_algorithm, 0.0650571428571425\n",
            "58750\n",
            "58750, reinforce_algorithm, 0.04502857142857133\n",
            "60000\n",
            "60000, reinforce_algorithm, 0.05535714285714281\n",
            "61250\n",
            "61250, reinforce_algorithm, 0.06498571428571401\n",
            "62500\n",
            "62500, reinforce_algorithm, 0.04514285714285701\n",
            "63750\n",
            "63750, reinforce_algorithm, 0.08512857142857116\n",
            "65000\n",
            "65000, reinforce_algorithm, 0.04532857142857133\n",
            "66250\n",
            "66250, reinforce_algorithm, 0.04511428571428547\n",
            "67500\n",
            "67500, reinforce_algorithm, 0.07519999999999945\n",
            "68750\n",
            "68750, reinforce_algorithm, 0.04502857142857151\n",
            "70000\n",
            "70000, reinforce_algorithm, 0.08495714285714273\n",
            "0\n",
            "0, reinforce_algorithm, -0.004228571428571442\n",
            "1250\n",
            "1250, reinforce_algorithm, 0.025857142857142953\n",
            "2500\n",
            "2500, reinforce_algorithm, 0.025414285714285948\n",
            "3750\n",
            "3750, reinforce_algorithm, 0.005771428571428558\n",
            "5000\n",
            "5000, reinforce_algorithm, 0.02574285714285716\n",
            "6250\n",
            "6250, reinforce_algorithm, 0.005657142857142825\n",
            "7500\n",
            "7500, reinforce_algorithm, 0.03557142857142872\n",
            "8750\n",
            "8750, reinforce_algorithm, 0.015557142857142905\n",
            "10000\n",
            "10000, reinforce_algorithm, 0.005471428571428521\n",
            "11250\n",
            "11250, reinforce_algorithm, 0.015414285714285665\n",
            "12500\n",
            "12500, reinforce_algorithm, 0.02558571428571431\n",
            "13750\n",
            "13750, reinforce_algorithm, 0.005685714285714271\n",
            "15000\n",
            "15000, reinforce_algorithm, 0.02591428571428582\n",
            "16250\n",
            "16250, reinforce_algorithm, 0.04545714285714297\n",
            "17500\n",
            "17500, reinforce_algorithm, 0.03547142857142874\n",
            "18750\n",
            "18750, reinforce_algorithm, 0.03555714285714305\n",
            "20000\n",
            "20000, reinforce_algorithm, 0.035442857142857345\n",
            "21250\n",
            "21250, reinforce_algorithm, 0.005499999999999974\n",
            "22500\n",
            "22500, reinforce_algorithm, 0.025014285714285687\n",
            "23750\n",
            "23750, reinforce_algorithm, 0.045728571428571126\n",
            "25000\n",
            "25000, reinforce_algorithm, 0.005514285714285698\n",
            "26250\n",
            "26250, reinforce_algorithm, 0.07505714285714256\n",
            "27500\n",
            "27500, reinforce_algorithm, 0.025500000000000217\n",
            "28750\n",
            "28750, reinforce_algorithm, 0.05549999999999959\n",
            "30000\n",
            "30000, reinforce_algorithm, 0.0650571428571425\n",
            "31250\n",
            "31250, reinforce_algorithm, 0.03525714285714305\n",
            "32500\n",
            "32500, reinforce_algorithm, 0.02535714285714303\n",
            "33750\n",
            "33750, reinforce_algorithm, 0.015414285714285941\n",
            "35000\n",
            "35000, reinforce_algorithm, 0.055185714285713956\n",
            "36250\n",
            "36250, reinforce_algorithm, 0.10515714285714328\n",
            "37500\n",
            "37500, reinforce_algorithm, 0.04558571428571376\n",
            "38750\n",
            "38750, reinforce_algorithm, 0.06531428571428545\n",
            "40000\n",
            "40000, reinforce_algorithm, 0.025571428571428734\n",
            "41250\n",
            "41250, reinforce_algorithm, 0.0254285714285715\n",
            "42500\n",
            "42500, reinforce_algorithm, 0.06521428571428514\n",
            "43750\n",
            "43750, reinforce_algorithm, 0.08525714285714323\n",
            "45000\n",
            "45000, reinforce_algorithm, 0.04517142857142788\n",
            "46250\n",
            "46250, reinforce_algorithm, 0.045328571428571295\n",
            "47500\n",
            "47500, reinforce_algorithm, 0.08529999999999946\n",
            "48750\n",
            "48750, reinforce_algorithm, 0.08527142857142823\n",
            "50000\n",
            "50000, reinforce_algorithm, 0.06524285714285687\n",
            "51250\n",
            "51250, reinforce_algorithm, 0.06514285714285696\n",
            "52500\n",
            "52500, reinforce_algorithm, 0.10492857142857154\n",
            "53750\n",
            "53750, reinforce_algorithm, 0.0752142857142853\n",
            "55000\n",
            "55000, reinforce_algorithm, 0.12478571428571432\n",
            "56250\n",
            "56250, reinforce_algorithm, 0.07488571428571351\n",
            "57500\n",
            "57500, reinforce_algorithm, 0.06515714285714273\n",
            "58750\n",
            "58750, reinforce_algorithm, 0.055271428571428104\n",
            "60000\n",
            "60000, reinforce_algorithm, 0.0853714285714285\n",
            "61250\n",
            "61250, reinforce_algorithm, 0.09507142857142796\n",
            "62500\n",
            "62500, reinforce_algorithm, 0.08501428571428564\n",
            "63750\n",
            "63750, reinforce_algorithm, 0.08509999999999991\n",
            "65000\n",
            "65000, reinforce_algorithm, 0.13461428571428574\n",
            "66250\n",
            "66250, reinforce_algorithm, 0.0550857142857142\n",
            "67500\n",
            "67500, reinforce_algorithm, 0.03520000000000016\n",
            "68750\n",
            "68750, reinforce_algorithm, 0.06538571428571369\n",
            "70000\n",
            "70000, reinforce_algorithm, 0.1246571428571438\n",
            "0\n",
            "0, A2C, 0.005471428571428545\n",
            "1250\n",
            "1250, A2C, 0.045342857142857185\n",
            "2500\n",
            "2500, A2C, 0.9814428571428507\n",
            "3750\n",
            "3750, A2C, -0.0010428571428571446\n",
            "5000\n",
            "5000, A2C, 0.9914285714285649\n",
            "6250\n",
            "6250, A2C, 0.9914285714285649\n",
            "7500\n",
            "7500, A2C, 0.9914285714285649\n",
            "8750\n",
            "8750, A2C, 0.9914285714285649\n",
            "10000\n",
            "10000, A2C, 0.9914285714285649\n",
            "11250\n",
            "11250, A2C, 0.9914285714285649\n",
            "12500\n",
            "12500, A2C, 0.9914285714285649\n",
            "13750\n",
            "13750, A2C, 0.9914285714285649\n",
            "15000\n",
            "15000, A2C, 0.9914285714285649\n",
            "16250\n",
            "16250, A2C, -0.0003285714285714286\n",
            "17500\n",
            "17500, A2C, 0.9814428571428507\n",
            "18750\n",
            "18750, A2C, 0.9914285714285649\n",
            "20000\n",
            "20000, A2C, 0.9914285714285649\n",
            "21250\n",
            "21250, A2C, 0.9914285714285649\n",
            "22500\n",
            "22500, A2C, 0.9914285714285649\n",
            "23750\n",
            "23750, A2C, 0.9814428571428506\n",
            "25000\n",
            "25000, A2C, 0.9914285714285649\n",
            "26250\n",
            "26250, A2C, 0.9914285714285649\n",
            "27500\n",
            "27500, A2C, 0.9914285714285649\n",
            "28750\n",
            "28750, A2C, 0.9914285714285649\n",
            "30000\n",
            "30000, A2C, 0.9914285714285649\n",
            "31250\n",
            "31250, A2C, 0.9914285714285649\n",
            "32500\n",
            "32500, A2C, 0.9914285714285649\n",
            "33750\n",
            "33750, A2C, 0.9914285714285649\n",
            "35000\n",
            "35000, A2C, 0.9914285714285649\n",
            "36250\n",
            "36250, A2C, -1.4285714285714285e-05\n",
            "37500\n",
            "37500, A2C, 0.9914285714285649\n",
            "38750\n",
            "38750, A2C, 0.9914285714285649\n",
            "40000\n",
            "40000, A2C, 0.9914285714285649\n",
            "41250\n",
            "41250, A2C, 0.9914285714285649\n",
            "42500\n",
            "42500, A2C, 0.9914285714285649\n",
            "43750\n",
            "43750, A2C, 0.9914285714285649\n",
            "45000\n",
            "45000, A2C, 0.9914285714285649\n",
            "46250\n",
            "46250, A2C, 0.9914285714285649\n",
            "47500\n",
            "47500, A2C, 0.9914285714285649\n",
            "48750\n",
            "48750, A2C, 0.9914285714285649\n",
            "50000\n",
            "50000, A2C, 0.9914285714285649\n",
            "51250\n",
            "51250, A2C, 0.9914285714285649\n",
            "52500\n",
            "52500, A2C, 0.9914285714285649\n",
            "53750\n",
            "53750, A2C, 0.9914285714285649\n",
            "55000\n",
            "55000, A2C, 0.9914285714285649\n",
            "56250\n",
            "56250, A2C, 0.9914285714285649\n",
            "57500\n",
            "57500, A2C, 0.9914285714285649\n",
            "58750\n",
            "58750, A2C, 0.9914285714285649\n",
            "60000\n",
            "60000, A2C, 0.9914285714285649\n",
            "61250\n",
            "61250, A2C, 0.9914285714285649\n",
            "62500\n",
            "62500, A2C, 0.9914285714285649\n",
            "63750\n",
            "63750, A2C, 0.9914285714285649\n",
            "65000\n",
            "65000, A2C, 0.9914285714285649\n",
            "66250\n",
            "66250, A2C, 0.9914285714285649\n",
            "67500\n",
            "67500, A2C, 0.9914285714285649\n",
            "68750\n",
            "68750, A2C, 0.9914285714285649\n",
            "70000\n",
            "70000, A2C, 0.9914285714285649\n",
            "0\n",
            "0, A2C, -0.0040142857142857254\n",
            "1250\n",
            "1250, A2C, 0.13450000000000106\n",
            "2500\n",
            "2500, A2C, 0.9914285714285649\n",
            "3750\n",
            "3750, A2C, 0.9914285714285649\n",
            "5000\n",
            "5000, A2C, -0.001485714285714286\n",
            "6250\n",
            "6250, A2C, 0.9814428571428508\n",
            "7500\n",
            "7500, A2C, 0.9914285714285649\n",
            "8750\n",
            "8750, A2C, 0.9914285714285649\n",
            "10000\n",
            "10000, A2C, 0.9914285714285649\n",
            "11250\n",
            "11250, A2C, 0.9914285714285649\n",
            "12500\n",
            "12500, A2C, 0.9914285714285649\n",
            "13750\n",
            "13750, A2C, 0.9914285714285649\n",
            "15000\n",
            "15000, A2C, 0.9914285714285649\n",
            "16250\n",
            "16250, A2C, 0.9914285714285649\n",
            "17500\n",
            "17500, A2C, 0.9914285714285649\n",
            "18750\n",
            "18750, A2C, 0.9914285714285649\n",
            "20000\n",
            "20000, A2C, 0.9914285714285649\n",
            "21250\n",
            "21250, A2C, 0.9914285714285649\n",
            "22500\n",
            "22500, A2C, 0.9914285714285649\n",
            "23750\n",
            "23750, A2C, 0.9914285714285649\n",
            "25000\n",
            "25000, A2C, 0.9914285714285649\n",
            "26250\n",
            "26250, A2C, 0.9914285714285649\n",
            "27500\n",
            "27500, A2C, 0.9914285714285649\n",
            "28750\n",
            "28750, A2C, 0.9914285714285649\n",
            "30000\n",
            "30000, A2C, 0.9914285714285649\n",
            "31250\n",
            "31250, A2C, 0.9914285714285649\n",
            "32500\n",
            "32500, A2C, 0.9914285714285649\n",
            "33750\n",
            "33750, A2C, 0.9914285714285649\n",
            "35000\n",
            "35000, A2C, 0.9914285714285649\n",
            "36250\n",
            "36250, A2C, 0.9914285714285649\n",
            "37500\n",
            "37500, A2C, 0.9914285714285649\n",
            "38750\n",
            "38750, A2C, 0.9914285714285649\n",
            "40000\n",
            "40000, A2C, 0.0\n",
            "41250\n",
            "41250, A2C, 0.9914285714285649\n",
            "42500\n",
            "42500, A2C, 0.9914285714285649\n",
            "43750\n",
            "43750, A2C, 0.9914285714285649\n",
            "45000\n",
            "45000, A2C, 0.9914285714285649\n",
            "46250\n",
            "46250, A2C, 0.9914285714285649\n",
            "47500\n",
            "47500, A2C, 0.9914285714285649\n",
            "48750\n",
            "48750, A2C, 0.9914285714285649\n",
            "50000\n",
            "50000, A2C, 0.9914285714285649\n",
            "51250\n",
            "51250, A2C, 0.9914285714285649\n",
            "52500\n",
            "52500, A2C, 0.0\n",
            "53750\n",
            "53750, A2C, 0.9914285714285649\n",
            "55000\n",
            "55000, A2C, 0.9914285714285649\n",
            "56250\n",
            "56250, A2C, 0.9914285714285649\n",
            "57500\n",
            "57500, A2C, 0.9914285714285649\n",
            "58750\n",
            "58750, A2C, 0.9914285714285649\n",
            "60000\n",
            "60000, A2C, 0.9914285714285649\n",
            "61250\n",
            "61250, A2C, 0.9914285714285649\n",
            "62500\n",
            "62500, A2C, 0.9914285714285649\n",
            "63750\n",
            "63750, A2C, 0.9914285714285649\n",
            "65000\n",
            "65000, A2C, 0.9914285714285649\n",
            "66250\n",
            "66250, A2C, 0.9914285714285649\n",
            "67500\n",
            "67500, A2C, 0.9914285714285649\n",
            "68750\n",
            "68750, A2C, 0.9914285714285649\n",
            "70000\n",
            "70000, A2C, 0.9914285714285649\n",
            "0\n",
            "0, A2C, 0.025485714285714497\n",
            "1250\n",
            "1250, A2C, 0.9814428571428508\n",
            "2500\n",
            "2500, A2C, 0.9914285714285649\n",
            "3750\n",
            "3750, A2C, 0.9814428571428507\n",
            "5000\n",
            "5000, A2C, 0.9914285714285649\n",
            "6250\n",
            "6250, A2C, 0.9914285714285649\n",
            "7500\n",
            "7500, A2C, 0.9814428571428508\n",
            "8750\n",
            "8750, A2C, 0.9914285714285649\n",
            "10000\n",
            "10000, A2C, 0.9914285714285649\n",
            "11250\n",
            "11250, A2C, 0.9914285714285649\n",
            "12500\n",
            "12500, A2C, 0.9914285714285649\n",
            "13750\n",
            "13750, A2C, 0.9914285714285649\n",
            "15000\n",
            "15000, A2C, 0.9914285714285649\n",
            "16250\n",
            "16250, A2C, 0.9914285714285649\n",
            "17500\n",
            "17500, A2C, 0.9914285714285649\n",
            "18750\n",
            "18750, A2C, 0.9914285714285649\n",
            "20000\n",
            "20000, A2C, 0.9914285714285649\n",
            "21250\n",
            "21250, A2C, 0.9914285714285649\n",
            "22500\n",
            "22500, A2C, 0.9914285714285649\n",
            "23750\n",
            "23750, A2C, 0.9814428571428507\n",
            "25000\n",
            "25000, A2C, 0.9914285714285649\n",
            "26250\n",
            "26250, A2C, 0.9914285714285649\n",
            "27500\n",
            "27500, A2C, 0.9914285714285649\n",
            "28750\n",
            "28750, A2C, 0.9914285714285649\n",
            "30000\n",
            "30000, A2C, 0.9914285714285649\n",
            "31250\n",
            "31250, A2C, 0.9914285714285649\n",
            "32500\n",
            "32500, A2C, 0.9914285714285649\n",
            "33750\n",
            "33750, A2C, 0.9914285714285649\n",
            "35000\n",
            "35000, A2C, 0.9914285714285649\n",
            "36250\n",
            "36250, A2C, 0.9914285714285649\n",
            "37500\n",
            "37500, A2C, 0.9914285714285649\n",
            "38750\n",
            "38750, A2C, 0.9914285714285649\n",
            "40000\n",
            "40000, A2C, 0.9914285714285649\n",
            "41250\n",
            "41250, A2C, 0.9914285714285649\n",
            "42500\n",
            "42500, A2C, 0.9914285714285649\n",
            "43750\n",
            "43750, A2C, 0.9914285714285649\n",
            "45000\n",
            "45000, A2C, 0.9914285714285649\n",
            "46250\n",
            "46250, A2C, 0.9914285714285649\n",
            "47500\n",
            "47500, A2C, 0.9914285714285649\n",
            "48750\n",
            "48750, A2C, 0.9914285714285649\n",
            "50000\n",
            "50000, A2C, 0.9914285714285649\n",
            "51250\n",
            "51250, A2C, 0.9914285714285649\n",
            "52500\n",
            "52500, A2C, 0.9914285714285649\n",
            "53750\n",
            "53750, A2C, 0.9914285714285649\n",
            "55000\n",
            "55000, A2C, 0.9914285714285649\n",
            "56250\n",
            "56250, A2C, 0.9914285714285649\n",
            "57500\n",
            "57500, A2C, 0.9914285714285649\n",
            "58750\n",
            "58750, A2C, 0.9914285714285649\n",
            "60000\n",
            "60000, A2C, 0.9914285714285649\n",
            "61250\n",
            "61250, A2C, 0.9914285714285649\n",
            "62500\n",
            "62500, A2C, 0.9914285714285649\n",
            "63750\n",
            "63750, A2C, 0.9914285714285649\n",
            "65000\n",
            "65000, A2C, 0.9914285714285649\n",
            "66250\n",
            "66250, A2C, 0.9914285714285649\n",
            "67500\n",
            "67500, A2C, 0.9914285714285649\n",
            "68750\n",
            "68750, A2C, 0.9914285714285649\n",
            "70000\n",
            "70000, A2C, 0.9914285714285649\n",
            "0\n",
            "0, PPO, 0.02538571428571455\n",
            "1250\n",
            "1250, PPO, 0.055228571428571016\n",
            "2500\n",
            "2500, PPO, 0.12474285714285785\n",
            "3750\n",
            "3750, PPO, 0.07524285714285699\n",
            "5000\n",
            "5000, PPO, 0.45335714285714424\n",
            "6250\n",
            "6250, PPO, 0.9714857142857081\n",
            "7500\n",
            "7500, PPO, 0.6826571428571455\n",
            "8750\n",
            "8750, PPO, 0.9715142857142797\n",
            "10000\n",
            "10000, PPO, 0.9514857142857093\n",
            "11250\n",
            "11250, PPO, 0.8715999999999967\n",
            "12500\n",
            "12500, PPO, 0.9914285714285649\n",
            "13750\n",
            "13750, PPO, 0.9914285714285649\n",
            "15000\n",
            "15000, PPO, 0.5720285714285765\n",
            "16250\n",
            "16250, PPO, 0.9914285714285649\n",
            "17500\n",
            "17500, PPO, 0.11389999999999992\n",
            "18750\n",
            "18750, PPO, 0.9914285714285649\n",
            "20000\n",
            "20000, PPO, 0.9914285714285649\n",
            "21250\n",
            "21250, PPO, 0.9814999999999937\n",
            "22500\n",
            "22500, PPO, 0.9914285714285649\n",
            "23750\n",
            "23750, PPO, 0.9914285714285649\n",
            "25000\n",
            "25000, PPO, 0.9914285714285649\n",
            "26250\n",
            "26250, PPO, 0.9714571428571367\n",
            "27500\n",
            "27500, PPO, 0.9914285714285649\n",
            "28750\n",
            "28750, PPO, 0.9914285714285649\n",
            "30000\n",
            "30000, PPO, 0.9914285714285649\n",
            "31250\n",
            "31250, PPO, 0.9914285714285649\n",
            "32500\n",
            "32500, PPO, 0.9914285714285649\n",
            "33750\n",
            "33750, PPO, 0.9914285714285649\n",
            "35000\n",
            "35000, PPO, 0.9914285714285649\n",
            "36250\n",
            "36250, PPO, 0.9914285714285649\n",
            "37500\n",
            "37500, PPO, 0.9914285714285649\n",
            "38750\n",
            "38750, PPO, 0.9914285714285649\n",
            "40000\n",
            "40000, PPO, 0.9914285714285649\n",
            "41250\n",
            "41250, PPO, 0.9614714285714224\n",
            "42500\n",
            "42500, PPO, 0.9914285714285649\n",
            "43750\n",
            "43750, PPO, 0.9914285714285649\n",
            "45000\n",
            "45000, PPO, 0.9814428571428507\n",
            "46250\n",
            "46250, PPO, 0.9914285714285649\n",
            "47500\n",
            "47500, PPO, 0.9914285714285649\n",
            "48750\n",
            "48750, PPO, 0.9914285714285649\n",
            "50000\n",
            "50000, PPO, 0.9015571428571377\n",
            "51250\n",
            "51250, PPO, 0.9814428571428508\n",
            "52500\n",
            "52500, PPO, 0.9914285714285649\n",
            "53750\n",
            "53750, PPO, 0.12267142857142849\n",
            "55000\n",
            "55000, PPO, 0.9914285714285649\n",
            "56250\n",
            "56250, PPO, 0.19568571428571416\n",
            "57500\n",
            "57500, PPO, 0.9914285714285649\n",
            "58750\n",
            "58750, PPO, 0.9914285714285649\n",
            "60000\n",
            "60000, PPO, 0.9914285714285649\n",
            "61250\n",
            "61250, PPO, 0.9315999999999949\n",
            "62500\n",
            "62500, PPO, 0.9914285714285649\n",
            "63750\n",
            "63750, PPO, 0.9914285714285649\n",
            "65000\n",
            "65000, PPO, 0.9914285714285649\n",
            "66250\n",
            "66250, PPO, 0.9914285714285649\n",
            "67500\n",
            "67500, PPO, 0.9914285714285649\n",
            "68750\n",
            "68750, PPO, 0.9914285714285649\n",
            "70000\n",
            "70000, PPO, 0.9914285714285649\n",
            "0\n",
            "0, PPO, -0.004557142857142874\n",
            "1250\n",
            "1250, PPO, 0.04512857142857141\n",
            "2500\n",
            "2500, PPO, 0.085471428571428\n",
            "3750\n",
            "3750, PPO, 0.18421428571428533\n",
            "5000\n",
            "5000, PPO, 0.3737285714285715\n",
            "6250\n",
            "6250, PPO, 0.9715142857142797\n",
            "7500\n",
            "7500, PPO, 0.9614999999999938\n",
            "8750\n",
            "8750, PPO, 0.9914285714285649\n",
            "10000\n",
            "10000, PPO, 0.9814428571428508\n",
            "11250\n",
            "11250, PPO, 0.96151428571428\n",
            "12500\n",
            "12500, PPO, 0.9814571428571365\n",
            "13750\n",
            "13750, PPO, 0.9914285714285649\n",
            "15000\n",
            "15000, PPO, 0.9914285714285649\n",
            "16250\n",
            "16250, PPO, 0.9914285714285649\n",
            "17500\n",
            "17500, PPO, 0.9914285714285649\n",
            "18750\n",
            "18750, PPO, 0.9814714285714221\n",
            "20000\n",
            "20000, PPO, 0.9914285714285649\n",
            "21250\n",
            "21250, PPO, 0.6019857142857186\n",
            "22500\n",
            "22500, PPO, 0.9814428571428507\n",
            "23750\n",
            "23750, PPO, 0.9914285714285649\n",
            "25000\n",
            "25000, PPO, 0.9914285714285649\n",
            "26250\n",
            "26250, PPO, 0.911571428571424\n",
            "27500\n",
            "27500, PPO, 0.9814428571428506\n",
            "28750\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import A2C, PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "def evaluate_policy(env, model, n_eval_episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(n_eval_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, _ = model.predict(state, deterministic=False)\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "    return total_reward / n_eval_episodes\n",
        "\n",
        "def run_algorithm(algo_class, env, total_steps, eval_interval, **algo_kwargs):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "\n",
        "    for step in range(0, total_steps + 1, eval_interval):\n",
        "        # Create a new instance of the algorithm for each evaluation\n",
        "        #print(step)\n",
        "        if algo_class in [A2C, PPO]:\n",
        "            algo = algo_class('MlpPolicy', env, **algo_kwargs)\n",
        "        else:\n",
        "            algo = algo_class(env, **algo_kwargs)\n",
        "\n",
        "        # Train the algorithm\n",
        "        algo.learn(total_timesteps=step)\n",
        "\n",
        "        # Evaluate the current policy\n",
        "        avg_reward = evaluate_policy(env, algo)\n",
        "        print(f\"{step}, {algo_class.__name__}, {avg_reward}\")\n",
        "        rewards.append(avg_reward)\n",
        "        steps.append(step)\n",
        "\n",
        "    return steps, rewards\n",
        "\n",
        "def run_multiple_times(algo_class, env, total_steps, eval_interval, num_runs=3, **algo_kwargs):\n",
        "    all_rewards = []\n",
        "    for _ in range(num_runs):\n",
        "        _, rewards = run_algorithm(algo_class, env, total_steps, eval_interval, **algo_kwargs)\n",
        "        all_rewards.append(rewards)\n",
        "\n",
        "    avg_rewards = np.mean(all_rewards, axis=0)\n",
        "    std_rewards = np.std(all_rewards, axis=0)\n",
        "    return avg_rewards, std_rewards\n",
        "\n",
        "# Set up the environment\n",
        "N = 7  # Set your desired N value\n",
        "env = gym.make('DeepSea-v0')\n",
        "\n",
        "# Set up algorithms with best hyperparameters\n",
        "total_steps = int(N*10000)\n",
        "eval_interval = int(total_steps // 56)\n",
        "\n",
        "# Run comparisons\n",
        "custom_tabular_rewards, custom_tabular_std = run_multiple_times(custom_algorithm, env, total_steps, eval_interval, method='tabular', batch_size=1250, epochs_per_batch=30, lr=0.01)\n",
        "custom_counting_rewards, custom_counting_std = run_multiple_times(custom_algorithm, env, total_steps, eval_interval, method='counting', batch_size=1250, epochs_per_batch=30, lr=0.01)\n",
        "reinforce_rewards, reinforce_std = run_multiple_times(reinforce_algorithm, env, total_steps, eval_interval, learning_rate=0.01, gamma=0.99)\n",
        "a2c_rewards, a2c_std = run_multiple_times(A2C, env, total_steps, eval_interval)\n",
        "ppo_rewards, ppo_std = run_multiple_times(PPO, env, total_steps, eval_interval)\n",
        "\n",
        "# Plot results\n",
        "steps = list(range(0, total_steps + 1, eval_interval))\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.plot(steps, custom_tabular_rewards, label='Tabular Algorithm')\n",
        "plt.fill_between(steps, custom_tabular_rewards - custom_tabular_std, custom_tabular_rewards + custom_tabular_std, alpha=0.3)\n",
        "\n",
        "plt.plot(steps, custom_counting_rewards, label='Counting Algorithm')\n",
        "plt.fill_between(steps, custom_counting_rewards - custom_counting_std, custom_counting_rewards + custom_counting_std, alpha=0.3)\n",
        "\n",
        "plt.plot(steps, reinforce_rewards, label='REINFORCE')\n",
        "plt.fill_between(steps, reinforce_rewards - reinforce_std, reinforce_rewards + reinforce_std, alpha=0.3)\n",
        "\n",
        "plt.plot(steps, a2c_rewards, label='A2C')\n",
        "plt.fill_between(steps, a2c_rewards - a2c_std, a2c_rewards + a2c_std, alpha=0.3)\n",
        "\n",
        "plt.plot(steps, ppo_rewards, label='PPO')\n",
        "plt.fill_between(steps, ppo_rewards - ppo_std, ppo_rewards + ppo_std, alpha=0.3)\n",
        "\n",
        "plt.xlabel('Number of Steps')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title(f'Algorithm Comparison for Deep Sea Exploration (N={N}, Average of 3 Runs)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}