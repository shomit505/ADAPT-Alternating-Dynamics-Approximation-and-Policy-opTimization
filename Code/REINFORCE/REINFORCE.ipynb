{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8ENrGikpZpk",
        "outputId": "303bbd8c-d608-491b-920a-0b77fb07d505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: farama-notifications, Mako, gymnasium, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 colorlog-6.8.2 farama-notifications-0.0.4 gymnasium-0.29.1 optuna-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa6SA-NXpQEB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class RiverSwimEnv(gym.Env):\n",
        "    def __init__(self, nS=6):\n",
        "        super(RiverSwimEnv, self).__init__()\n",
        "        self.nS = nS\n",
        "        self.nA = 2  # LEFT = 0, RIGHT = 1\n",
        "        self.state = 0\n",
        "        self.steps_taken = 0\n",
        "        self.max_steps = 20\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.nA)\n",
        "        self.observation_space = spaces.Discrete(self.nS)\n",
        "\n",
        "        # Define transition probabilities and rewards\n",
        "        self.P = self._init_dynamics()\n",
        "\n",
        "    def _init_dynamics(self):\n",
        "        P = {}\n",
        "        for s in range(self.nS):\n",
        "            P[s] = {a: [] for a in range(self.nA)}\n",
        "\n",
        "        # LEFT transitions\n",
        "        for s in range(self.nS):\n",
        "            P[s][0] = [(1.0, max(0, s-1), 5/1000 if s == 0 else 0, False)]\n",
        "\n",
        "        # RIGHT transitions\n",
        "        P[0][1] = [(0.3, 0, 0, False), (0.7, 1, 0, False)]\n",
        "        for s in range(1, self.nS - 1):\n",
        "            P[s][1] = [\n",
        "                (0.1, max(0, s-1), 0, False),\n",
        "                (0.6, s, 0, False),\n",
        "                (0.3, min(self.nS-1, s+1), 0, False)\n",
        "            ]\n",
        "        P[self.nS-1][1] = [(0.7, self.nS-1, 1, False), (0.3, self.nS-2, 0, False)]\n",
        "\n",
        "        return P\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.state = 0\n",
        "        self.steps_taken = 0\n",
        "        return self.state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        transitions = self.P[self.state][action]\n",
        "        i = self.np_random.choice(len(transitions), p=[t[0] for t in transitions])\n",
        "        p, next_state, reward, _ = transitions[i]\n",
        "        self.state = next_state\n",
        "        self.steps_taken += 1\n",
        "\n",
        "        # Check if max steps reached\n",
        "        done = self.steps_taken >= self.max_steps\n",
        "\n",
        "        return next_state, reward, done, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Current state: {self.state}\")\n",
        "\n",
        "# Register the environment\n",
        "gym.register(\n",
        "    id='RiverSwim-v0',\n",
        "    entry_point='__main__:RiverSwimEnv',\n",
        "    max_episode_steps=20,\n",
        ")\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('RiverSwim-v0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env= gym.make('FrozenLake-v1', is_slippery=False)"
      ],
      "metadata": {
        "id": "JOLMd-Yh4eCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "class REINFORCE:\n",
        "    def __init__(self, env, learning_rate=0.001, gamma=0.99):\n",
        "        self.env = env\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.n_actions = env.action_space.n\n",
        "        #self.N = env.N  # Assuming the environment has an attribute N for grid size\n",
        "        self.n_states = env.observation_space.n\n",
        "        self.action_preferences = np.zeros((self.n_states, self.n_actions))\n",
        "\n",
        "    def square_max_policy(self, state):\n",
        "        preferences = self.action_preferences[state]\n",
        "        exp_preferences = np.exp(preferences - np.max(preferences))\n",
        "        return exp_preferences**2 / np.sum(exp_preferences**2)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        policy = self.square_max_policy(state)\n",
        "        return np.random.choice(self.n_actions, p=policy)\n",
        "\n",
        "    def update_policy(self, episode):\n",
        "        G = 0\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            policy = self.square_max_policy(state)\n",
        "\n",
        "            # Compute the gradient\n",
        "            grad = np.zeros(self.n_actions)\n",
        "            for a in range(self.n_actions):\n",
        "                if a == action:\n",
        "                    grad[a] = 2 * policy[a] * (1 - policy[a])\n",
        "                else:\n",
        "                    grad[a] = -2 * policy[a] * policy[action]\n",
        "\n",
        "            # Update action preferences\n",
        "            self.action_preferences[state] += self.lr * G * grad\n",
        "\n",
        "    def train(self, n_episodes):\n",
        "        for _ in range(n_episodes):\n",
        "            state, _ = self.env.reset()\n",
        "            episode = []\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "                episode.append((state, action, reward))\n",
        "                state = next_state\n",
        "            self.update_policy(episode)\n",
        "\n",
        "def evaluate_agent(env, agent, n_eval_episodes):\n",
        "    total_reward = 0\n",
        "    for _ in range(n_eval_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "    return total_reward / n_eval_episodes\n",
        "\n",
        "def run_experiment(env, n_episodes, eval_freq, n_eval_episodes):\n",
        "    agent = REINFORCE(env)\n",
        "    results = []\n",
        "\n",
        "    iterations = n_episodes // eval_freq\n",
        "    print(iterations)\n",
        "    for iteration in range(iterations):\n",
        "        agent.train(eval_freq)\n",
        "        avg_reward = evaluate_agent(env, agent, n_eval_episodes)\n",
        "        step_count = (iteration + 1) * eval_freq * 20\n",
        "        print(step_count)\n",
        "        results.append((step_count, avg_reward))\n",
        "        print(f\"Iteration {iteration + 1}, Average Reward: {avg_reward:.2f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Experiment parameters\n",
        "n_episodes = 10000\n",
        "eval_freq = 51\n",
        "n_eval_episodes = 100\n",
        "num_runs = 1\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "for run in range(num_runs):\n",
        "    print(f\"Starting run {run + 1}/{num_runs}\")\n",
        "    env = gym.make('RiverSwim-v0')\n",
        "    run_results = run_experiment(env, n_episodes, eval_freq, n_eval_episodes)\n",
        "    all_results.append(run_results)\n",
        "    env.close()\n",
        "\n",
        "# Process results\n",
        "step_sizes = [result[0] for result in all_results[0]]  # Assuming all runs have the same step sizes\n",
        "print(step_sizes)\n",
        "averaged_rewards = []\n",
        "\n",
        "for i in range(len(step_sizes)):\n",
        "    rewards_at_episode = [run[i][1] for run in all_results]\n",
        "    avg_reward = np.mean(rewards_at_episode)\n",
        "    averaged_rewards.append(avg_reward)\n",
        "\n",
        "# Create the final list of tuples (episode, averaged_reward)\n",
        "final_results = list(zip(step_sizes, averaged_rewards))\n",
        "\n",
        "# Save results to CSV file\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"reinforce_squaremax_riversim_results_{timestamp}.csv\"\n",
        "\n",
        "with open(filename, 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(['Step', 'Average Reward'])  # Write header\n",
        "    csvwriter.writerows(final_results)  # Write data\n",
        "\n",
        "print(f\"Results saved to {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPD0ehd8u139",
        "outputId": "f1ca5508-9222-43a8-c21a-db9af255cb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting run 1/1\n",
            "196\n",
            "1020\n",
            "Iteration 1, Average Reward: 0.03\n",
            "2040\n",
            "Iteration 2, Average Reward: 0.07\n",
            "3060\n",
            "Iteration 3, Average Reward: 0.03\n",
            "4080\n",
            "Iteration 4, Average Reward: 0.03\n",
            "5100\n",
            "Iteration 5, Average Reward: 0.04\n",
            "6120\n",
            "Iteration 6, Average Reward: 0.07\n",
            "7140\n",
            "Iteration 7, Average Reward: 0.03\n",
            "8160\n",
            "Iteration 8, Average Reward: 0.04\n",
            "9180\n",
            "Iteration 9, Average Reward: 0.05\n",
            "10200\n",
            "Iteration 10, Average Reward: 0.06\n",
            "11220\n",
            "Iteration 11, Average Reward: 0.03\n",
            "12240\n",
            "Iteration 12, Average Reward: 0.04\n",
            "13260\n",
            "Iteration 13, Average Reward: 0.03\n",
            "14280\n",
            "Iteration 14, Average Reward: 0.03\n",
            "15300\n",
            "Iteration 15, Average Reward: 0.03\n",
            "16320\n",
            "Iteration 16, Average Reward: 0.03\n",
            "17340\n",
            "Iteration 17, Average Reward: 0.03\n",
            "18360\n",
            "Iteration 18, Average Reward: 0.03\n",
            "19380\n",
            "Iteration 19, Average Reward: 0.04\n",
            "20400\n",
            "Iteration 20, Average Reward: 0.03\n",
            "21420\n",
            "Iteration 21, Average Reward: 0.03\n",
            "22440\n",
            "Iteration 22, Average Reward: 0.03\n",
            "23460\n",
            "Iteration 23, Average Reward: 0.03\n",
            "24480\n",
            "Iteration 24, Average Reward: 0.05\n",
            "25500\n",
            "Iteration 25, Average Reward: 0.03\n",
            "26520\n",
            "Iteration 26, Average Reward: 0.04\n",
            "27540\n",
            "Iteration 27, Average Reward: 0.03\n",
            "28560\n",
            "Iteration 28, Average Reward: 0.03\n",
            "29580\n",
            "Iteration 29, Average Reward: 0.05\n",
            "30600\n",
            "Iteration 30, Average Reward: 0.03\n",
            "31620\n",
            "Iteration 31, Average Reward: 0.03\n",
            "32640\n",
            "Iteration 32, Average Reward: 0.04\n",
            "33660\n",
            "Iteration 33, Average Reward: 0.04\n",
            "34680\n",
            "Iteration 34, Average Reward: 0.03\n",
            "35700\n",
            "Iteration 35, Average Reward: 0.03\n",
            "36720\n",
            "Iteration 36, Average Reward: 0.03\n",
            "37740\n",
            "Iteration 37, Average Reward: 0.03\n",
            "38760\n",
            "Iteration 38, Average Reward: 0.03\n",
            "39780\n",
            "Iteration 39, Average Reward: 0.05\n",
            "40800\n",
            "Iteration 40, Average Reward: 0.03\n",
            "41820\n",
            "Iteration 41, Average Reward: 0.05\n",
            "42840\n",
            "Iteration 42, Average Reward: 0.03\n",
            "43860\n",
            "Iteration 43, Average Reward: 0.03\n",
            "44880\n",
            "Iteration 44, Average Reward: 0.04\n",
            "45900\n",
            "Iteration 45, Average Reward: 0.04\n",
            "46920\n",
            "Iteration 46, Average Reward: 0.03\n",
            "47940\n",
            "Iteration 47, Average Reward: 0.03\n",
            "48960\n",
            "Iteration 48, Average Reward: 0.03\n",
            "49980\n",
            "Iteration 49, Average Reward: 0.05\n",
            "51000\n",
            "Iteration 50, Average Reward: 0.05\n",
            "52020\n",
            "Iteration 51, Average Reward: 0.04\n",
            "53040\n",
            "Iteration 52, Average Reward: 0.04\n",
            "54060\n",
            "Iteration 53, Average Reward: 0.03\n",
            "55080\n",
            "Iteration 54, Average Reward: 0.03\n",
            "56100\n",
            "Iteration 55, Average Reward: 0.04\n",
            "57120\n",
            "Iteration 56, Average Reward: 0.04\n",
            "58140\n",
            "Iteration 57, Average Reward: 0.03\n",
            "59160\n",
            "Iteration 58, Average Reward: 0.03\n",
            "60180\n",
            "Iteration 59, Average Reward: 0.04\n",
            "61200\n",
            "Iteration 60, Average Reward: 0.04\n",
            "62220\n",
            "Iteration 61, Average Reward: 0.06\n",
            "63240\n",
            "Iteration 62, Average Reward: 0.03\n",
            "64260\n",
            "Iteration 63, Average Reward: 0.04\n",
            "65280\n",
            "Iteration 64, Average Reward: 0.05\n",
            "66300\n",
            "Iteration 65, Average Reward: 0.04\n",
            "67320\n",
            "Iteration 66, Average Reward: 0.04\n",
            "68340\n",
            "Iteration 67, Average Reward: 0.04\n",
            "69360\n",
            "Iteration 68, Average Reward: 0.04\n",
            "70380\n",
            "Iteration 69, Average Reward: 0.04\n",
            "71400\n",
            "Iteration 70, Average Reward: 0.04\n",
            "72420\n",
            "Iteration 71, Average Reward: 0.04\n",
            "73440\n",
            "Iteration 72, Average Reward: 0.04\n",
            "74460\n",
            "Iteration 73, Average Reward: 0.06\n",
            "75480\n",
            "Iteration 74, Average Reward: 0.04\n",
            "76500\n",
            "Iteration 75, Average Reward: 0.04\n",
            "77520\n",
            "Iteration 76, Average Reward: 0.04\n",
            "78540\n",
            "Iteration 77, Average Reward: 0.05\n",
            "79560\n",
            "Iteration 78, Average Reward: 0.04\n",
            "80580\n",
            "Iteration 79, Average Reward: 0.06\n",
            "81600\n",
            "Iteration 80, Average Reward: 0.04\n",
            "82620\n",
            "Iteration 81, Average Reward: 0.04\n",
            "83640\n",
            "Iteration 82, Average Reward: 0.04\n",
            "84660\n",
            "Iteration 83, Average Reward: 0.04\n",
            "85680\n",
            "Iteration 84, Average Reward: 0.05\n",
            "86700\n",
            "Iteration 85, Average Reward: 0.05\n",
            "87720\n",
            "Iteration 86, Average Reward: 0.06\n",
            "88740\n",
            "Iteration 87, Average Reward: 0.04\n",
            "89760\n",
            "Iteration 88, Average Reward: 0.05\n",
            "90780\n",
            "Iteration 89, Average Reward: 0.05\n",
            "91800\n",
            "Iteration 90, Average Reward: 0.05\n",
            "92820\n",
            "Iteration 91, Average Reward: 0.05\n",
            "93840\n",
            "Iteration 92, Average Reward: 0.05\n",
            "94860\n",
            "Iteration 93, Average Reward: 0.06\n",
            "95880\n",
            "Iteration 94, Average Reward: 0.05\n",
            "96900\n",
            "Iteration 95, Average Reward: 0.08\n",
            "97920\n",
            "Iteration 96, Average Reward: 0.05\n",
            "98940\n",
            "Iteration 97, Average Reward: 0.05\n",
            "99960\n",
            "Iteration 98, Average Reward: 0.05\n",
            "100980\n",
            "Iteration 99, Average Reward: 0.09\n",
            "102000\n",
            "Iteration 100, Average Reward: 0.05\n",
            "103020\n",
            "Iteration 101, Average Reward: 0.05\n",
            "104040\n",
            "Iteration 102, Average Reward: 0.05\n",
            "105060\n",
            "Iteration 103, Average Reward: 0.05\n",
            "106080\n",
            "Iteration 104, Average Reward: 0.05\n",
            "107100\n",
            "Iteration 105, Average Reward: 0.05\n",
            "108120\n",
            "Iteration 106, Average Reward: 0.05\n",
            "109140\n",
            "Iteration 107, Average Reward: 0.05\n",
            "110160\n",
            "Iteration 108, Average Reward: 0.06\n",
            "111180\n",
            "Iteration 109, Average Reward: 0.08\n",
            "112200\n",
            "Iteration 110, Average Reward: 0.07\n",
            "113220\n",
            "Iteration 111, Average Reward: 0.06\n",
            "114240\n",
            "Iteration 112, Average Reward: 0.06\n",
            "115260\n",
            "Iteration 113, Average Reward: 0.08\n",
            "116280\n",
            "Iteration 114, Average Reward: 0.06\n",
            "117300\n",
            "Iteration 115, Average Reward: 0.06\n",
            "118320\n",
            "Iteration 116, Average Reward: 0.06\n",
            "119340\n",
            "Iteration 117, Average Reward: 0.06\n",
            "120360\n",
            "Iteration 118, Average Reward: 0.06\n",
            "121380\n",
            "Iteration 119, Average Reward: 0.07\n",
            "122400\n",
            "Iteration 120, Average Reward: 0.07\n",
            "123420\n",
            "Iteration 121, Average Reward: 0.12\n",
            "124440\n",
            "Iteration 122, Average Reward: 0.06\n",
            "125460\n",
            "Iteration 123, Average Reward: 0.07\n",
            "126480\n",
            "Iteration 124, Average Reward: 0.07\n",
            "127500\n",
            "Iteration 125, Average Reward: 0.07\n",
            "128520\n",
            "Iteration 126, Average Reward: 0.09\n",
            "129540\n",
            "Iteration 127, Average Reward: 0.10\n",
            "130560\n",
            "Iteration 128, Average Reward: 0.06\n",
            "131580\n",
            "Iteration 129, Average Reward: 0.07\n",
            "132600\n",
            "Iteration 130, Average Reward: 0.07\n",
            "133620\n",
            "Iteration 131, Average Reward: 0.07\n",
            "134640\n",
            "Iteration 132, Average Reward: 0.07\n",
            "135660\n",
            "Iteration 133, Average Reward: 0.07\n",
            "136680\n",
            "Iteration 134, Average Reward: 0.07\n",
            "137700\n",
            "Iteration 135, Average Reward: 0.09\n",
            "138720\n",
            "Iteration 136, Average Reward: 0.07\n",
            "139740\n",
            "Iteration 137, Average Reward: 0.09\n",
            "140760\n",
            "Iteration 138, Average Reward: 0.08\n",
            "141780\n",
            "Iteration 139, Average Reward: 0.09\n",
            "142800\n",
            "Iteration 140, Average Reward: 0.08\n",
            "143820\n",
            "Iteration 141, Average Reward: 0.07\n",
            "144840\n",
            "Iteration 142, Average Reward: 0.08\n",
            "145860\n",
            "Iteration 143, Average Reward: 0.09\n",
            "146880\n",
            "Iteration 144, Average Reward: 0.07\n",
            "147900\n",
            "Iteration 145, Average Reward: 0.07\n",
            "148920\n",
            "Iteration 146, Average Reward: 0.07\n",
            "149940\n",
            "Iteration 147, Average Reward: 0.08\n",
            "150960\n",
            "Iteration 148, Average Reward: 0.08\n",
            "151980\n",
            "Iteration 149, Average Reward: 0.08\n",
            "153000\n",
            "Iteration 150, Average Reward: 0.08\n",
            "154020\n",
            "Iteration 151, Average Reward: 0.08\n",
            "155040\n",
            "Iteration 152, Average Reward: 0.08\n",
            "156060\n",
            "Iteration 153, Average Reward: 0.08\n",
            "157080\n",
            "Iteration 154, Average Reward: 0.08\n",
            "158100\n",
            "Iteration 155, Average Reward: 0.08\n",
            "159120\n",
            "Iteration 156, Average Reward: 0.08\n",
            "160140\n",
            "Iteration 157, Average Reward: 0.08\n",
            "161160\n",
            "Iteration 158, Average Reward: 0.10\n",
            "162180\n",
            "Iteration 159, Average Reward: 0.08\n",
            "163200\n",
            "Iteration 160, Average Reward: 0.08\n",
            "164220\n",
            "Iteration 161, Average Reward: 0.08\n",
            "165240\n",
            "Iteration 162, Average Reward: 0.10\n",
            "166260\n",
            "Iteration 163, Average Reward: 0.08\n",
            "167280\n",
            "Iteration 164, Average Reward: 0.08\n",
            "168300\n",
            "Iteration 165, Average Reward: 0.08\n",
            "169320\n",
            "Iteration 166, Average Reward: 0.08\n",
            "170340\n",
            "Iteration 167, Average Reward: 0.08\n",
            "171360\n",
            "Iteration 168, Average Reward: 0.08\n",
            "172380\n",
            "Iteration 169, Average Reward: 0.09\n",
            "173400\n",
            "Iteration 170, Average Reward: 0.09\n",
            "174420\n",
            "Iteration 171, Average Reward: 0.09\n",
            "175440\n",
            "Iteration 172, Average Reward: 0.08\n",
            "176460\n",
            "Iteration 173, Average Reward: 0.08\n",
            "177480\n",
            "Iteration 174, Average Reward: 0.09\n",
            "178500\n",
            "Iteration 175, Average Reward: 0.08\n",
            "179520\n",
            "Iteration 176, Average Reward: 0.13\n",
            "180540\n",
            "Iteration 177, Average Reward: 0.09\n",
            "181560\n",
            "Iteration 178, Average Reward: 0.09\n",
            "182580\n",
            "Iteration 179, Average Reward: 0.08\n",
            "183600\n",
            "Iteration 180, Average Reward: 0.08\n",
            "184620\n",
            "Iteration 181, Average Reward: 0.09\n",
            "185640\n",
            "Iteration 182, Average Reward: 0.09\n",
            "186660\n",
            "Iteration 183, Average Reward: 0.09\n",
            "187680\n",
            "Iteration 184, Average Reward: 0.09\n",
            "188700\n",
            "Iteration 185, Average Reward: 0.09\n",
            "189720\n",
            "Iteration 186, Average Reward: 0.09\n",
            "190740\n",
            "Iteration 187, Average Reward: 0.12\n",
            "191760\n",
            "Iteration 188, Average Reward: 0.09\n",
            "192780\n",
            "Iteration 189, Average Reward: 0.08\n",
            "193800\n",
            "Iteration 190, Average Reward: 0.09\n",
            "194820\n",
            "Iteration 191, Average Reward: 0.09\n",
            "195840\n",
            "Iteration 192, Average Reward: 0.09\n",
            "196860\n",
            "Iteration 193, Average Reward: 0.09\n",
            "197880\n",
            "Iteration 194, Average Reward: 0.09\n",
            "198900\n",
            "Iteration 195, Average Reward: 0.09\n",
            "199920\n",
            "Iteration 196, Average Reward: 0.09\n",
            "[1020, 2040, 3060, 4080, 5100, 6120, 7140, 8160, 9180, 10200, 11220, 12240, 13260, 14280, 15300, 16320, 17340, 18360, 19380, 20400, 21420, 22440, 23460, 24480, 25500, 26520, 27540, 28560, 29580, 30600, 31620, 32640, 33660, 34680, 35700, 36720, 37740, 38760, 39780, 40800, 41820, 42840, 43860, 44880, 45900, 46920, 47940, 48960, 49980, 51000, 52020, 53040, 54060, 55080, 56100, 57120, 58140, 59160, 60180, 61200, 62220, 63240, 64260, 65280, 66300, 67320, 68340, 69360, 70380, 71400, 72420, 73440, 74460, 75480, 76500, 77520, 78540, 79560, 80580, 81600, 82620, 83640, 84660, 85680, 86700, 87720, 88740, 89760, 90780, 91800, 92820, 93840, 94860, 95880, 96900, 97920, 98940, 99960, 100980, 102000, 103020, 104040, 105060, 106080, 107100, 108120, 109140, 110160, 111180, 112200, 113220, 114240, 115260, 116280, 117300, 118320, 119340, 120360, 121380, 122400, 123420, 124440, 125460, 126480, 127500, 128520, 129540, 130560, 131580, 132600, 133620, 134640, 135660, 136680, 137700, 138720, 139740, 140760, 141780, 142800, 143820, 144840, 145860, 146880, 147900, 148920, 149940, 150960, 151980, 153000, 154020, 155040, 156060, 157080, 158100, 159120, 160140, 161160, 162180, 163200, 164220, 165240, 166260, 167280, 168300, 169320, 170340, 171360, 172380, 173400, 174420, 175440, 176460, 177480, 178500, 179520, 180540, 181560, 182580, 183600, 184620, 185640, 186660, 187680, 188700, 189720, 190740, 191760, 192780, 193800, 194820, 195840, 196860, 197880, 198900, 199920]\n",
            "Results saved to reinforce_squaremax_riversim_results_20241006_113914.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "class REINFORCE:\n",
        "    def __init__(self, env, learning_rate=0.001, gamma=0.99):\n",
        "        self.env = env\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.n_actions = env.action_space.n\n",
        "        self.n_states = env.observation_space.n\n",
        "        self.action_preferences = np.zeros((self.n_states, self.n_actions))\n",
        "\n",
        "    def square_max_policy(self, state):\n",
        "        preferences = self.action_preferences[state]\n",
        "        exp_preferences = np.exp(preferences - np.max(preferences))\n",
        "        return exp_preferences**2 / np.sum(exp_preferences**2)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        policy = self.square_max_policy(state)\n",
        "        return np.random.choice(self.n_actions, p=policy)\n",
        "\n",
        "    def update_policy(self, episode):\n",
        "        G = 0\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            policy = self.square_max_policy(state)\n",
        "\n",
        "            grad = np.zeros(self.n_actions)\n",
        "            for a in range(self.n_actions):\n",
        "                if a == action:\n",
        "                    grad[a] = 2 * policy[a] * (1 - policy[a])\n",
        "                else:\n",
        "                    grad[a] = -2 * policy[a] * policy[action]\n",
        "\n",
        "            self.action_preferences[state] += self.lr * G * grad\n",
        "\n",
        "    def train(self, n_steps):\n",
        "        steps_taken = 0\n",
        "        while steps_taken < n_steps:\n",
        "            state, _ = self.env.reset()\n",
        "            episode = []\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "                episode.append((state, action, reward))\n",
        "                state = next_state\n",
        "                steps_taken += 1\n",
        "                if steps_taken >= n_steps:\n",
        "                    break\n",
        "            self.update_policy(episode)\n",
        "        return steps_taken\n",
        "\n",
        "def evaluate_agent(env, agent, n_eval_episodes):\n",
        "    total_reward = 0\n",
        "    for _ in range(n_eval_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "        total_reward += episode_reward\n",
        "    return total_reward / n_eval_episodes\n",
        "\n",
        "def run_experiment(env, total_steps, eval_freq, n_eval_episodes):\n",
        "    agent = REINFORCE(env)\n",
        "    results = []\n",
        "    steps_taken = 0\n",
        "\n",
        "    while steps_taken < total_steps:\n",
        "        steps_in_iteration = agent.train(eval_freq)\n",
        "        steps_taken += steps_in_iteration\n",
        "        avg_reward = evaluate_agent(env, agent, n_eval_episodes)\n",
        "        results.append((steps_taken, avg_reward))\n",
        "        print(f\"Steps: {steps_taken}, Average Reward: {avg_reward:.2f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Experiment parameters\n",
        "total_steps = 200000\n",
        "eval_freq = 1000\n",
        "n_eval_episodes = 100\n",
        "num_runs = 1\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "for run in range(num_runs):\n",
        "    print(f\"Starting run {run + 1}/{num_runs}\")\n",
        "    env = gym.make('RiverSwim-v0')\n",
        "    run_results = run_experiment(env, total_steps, eval_freq, n_eval_episodes)\n",
        "    all_results.append(run_results)\n",
        "    env.close()\n",
        "\n",
        "# Process results\n",
        "step_sizes = [result[0] for result in all_results[0]]\n",
        "averaged_rewards = []\n",
        "\n",
        "for i in range(len(step_sizes)):\n",
        "    rewards_at_step = [run[i][1] for run in all_results]\n",
        "    avg_reward = np.mean(rewards_at_step)\n",
        "    averaged_rewards.append(avg_reward)\n",
        "\n",
        "# Create the final list of tuples (steps, averaged_reward)\n",
        "final_results = list(zip(step_sizes, averaged_rewards))\n",
        "\n",
        "# Save results to CSV file\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"reinforce_squaremax_riversim_results_{timestamp}.csv\"\n",
        "\n",
        "with open(filename, 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(['Steps', 'Average Reward'])  # Write header\n",
        "    csvwriter.writerows(final_results)  # Write data\n",
        "\n",
        "print(f\"Results saved to {filename}\")"
      ],
      "metadata": {
        "id": "FHMr5N7sV6Hh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}